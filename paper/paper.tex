\documentclass{article}

\usepackage{amsmath, amsfonts, microtype, xcolor, tikz, graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage[]{neurips_2019}
\DeclareMathOperator{\Tr}{Tr}

\usetikzlibrary{calc}

\tikzset{
    ncbar angle/.initial=90,
    ncbar/.style={
        to path=(\tikztostart)
        -- ($(\tikztostart)!#1!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztotarget)$)
        -- ($(\tikztotarget)!($(\tikztostart)!#1!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztotarget)$)!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztostart)$)
        -- (\tikztotarget)
    },
    ncbar/.default=0.5cm,
}

\tikzset{round left paren/.style={ncbar=0.5cm,out=110,in=-110}}
\tikzset{round right paren/.style={ncbar=0.5cm,out=70,in=-70}}

\newcommand{\dlp}[1]{{\color{red} (DLP: #1)}}
\newcommand{\mo}[1]{{\color{green} (MO: #1)}}
\newcommand{\jrk}[1]{{\color{blue} (JRK: #1)}}
\newcommand{\fc}[1]{{\color{orange} (FC: #1)}}

\title{Discovering causal factors with back-to-back regression}

\author{%
  Jean-Remi King\\
  CNRS\\
  \texttt{email} \\
  \And
  Fran\c{c}ois Charton\\
  Facebook AI\\
  \texttt{fcharton@fb.com}\\
  \And
  David Lopez-Paz\\
  Facebook AI\\
  \texttt{dlp@fb.com}
  \And
  Maxime Oquab\\
  Facebook AI\\
  \texttt{qas@fb.com}
}

\begin{document}

\maketitle

\begin{abstract}
Identifying causes from observations is at the core of science. This endeavor
is particularly challenging when i) potential factors are difficult to
manipulate individually and ii) observations are complex and multi-dimensional.
To address this issue, we introduce ``back-to-back regression'' (BFR), a
method designed to identify, from a set of co-varying factors, the factors
that most plausible account for multidimensional observations. After detailing
the proof of convergence, we show that BFR outperforms least-squares and related
regression techniques, as well as cross-decomposition methods (e.g. canonical
correlation analysis, and partial least squares) on two tasks: causal
identification and out-of-distribution prediction. Finally, we apply BFR to
neuroimaging recordings of 102 subjects reading word sequences. The results
show, as expected, that the early brain responses to words appear to be
specifically caused by low-level visual factors whereas late brain responses
appear to be caused by lexical factors, despite the fact that these factors
covary in the study.
% conclude  open
\end{abstract}

\section{Introduction}

\input{introduction.tex}

\section{Back to back regression}

\begin{figure}[t!]
    \centering
    \begin{tikzpicture}
    \newcommand\posY{0}
    \newcommand\posX{3}
    \newcommand\posE{5}
    \newcommand\posN{7}
    \newcommand\posF{9.5}

    \node[thick, draw=black, minimum height=3cm, minimum width=2cm, fill=yellow!10] (Y) at (\posY, 0){};
    \node[] (eq) at (1.5, 0){$=$};
    \node[] (times) at (4, 0){$\times$};
    \node[thick, draw=black, minimum height=3cm, minimum width=1cm, fill=yellow!10] (X) at (\posX, 0){};
    \node[thick, draw=black, minimum height=1cm, minimum width=1cm, fill=red!10] (E) at (\posE, 0){};

    \draw[fill=white] (\posE - 0.5 + 0.0, 0.5 - 0.0) rectangle (\posE - 0.5 + 0.0 + 0.1, 0.5 - 0.0 - 0.1);
    \draw[fill=black] (\posE - 0.5 + 0.1, 0.5 - 0.1) rectangle (\posE - 0.5 + 0.1 + 0.1, 0.5 - 0.1 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.2, 0.5 - 0.2) rectangle (\posE - 0.5 + 0.2 + 0.1, 0.5 - 0.2 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.3, 0.5 - 0.3) rectangle (\posE - 0.5 + 0.3 + 0.1, 0.5 - 0.3 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.4, 0.5 - 0.4) rectangle (\posE - 0.5 + 0.4 + 0.1, 0.5 - 0.4 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.5, 0.5 - 0.5) rectangle (\posE - 0.5 + 0.5 + 0.1, 0.5 - 0.5 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.6, 0.5 - 0.6) rectangle (\posE - 0.5 + 0.6 + 0.1, 0.5 - 0.6 - 0.1);
    \draw[fill=black] (\posE - 0.5 + 0.7, 0.5 - 0.7) rectangle (\posE - 0.5 + 0.7 + 0.1, 0.5 - 0.7 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.8, 0.5 - 0.8) rectangle (\posE - 0.5 + 0.8 + 0.1, 0.5 - 0.8 - 0.1);
    \draw[fill=black] (\posE - 0.5 + 0.9, 0.5 - 0.9) rectangle (\posE - 0.5 + 0.9 + 0.1, 0.5 - 0.9 - 0.1);

    \node[] (plus) at (6, 0){$+$};
    \node[thick, draw=black, minimum height=3cm, minimum width=1cm, fill=blue!10] (N) at (\posN, 0){};
    \node[thick, draw=black, minimum height=2cm, minimum width=1cm, fill=red!10] (F) at (\posF, 0){};
    \draw [thick] (2.25, -1.5) to [round left paren ] (2.25, 1.5);
    \draw [thick] (7.75, -1.5) to [round right paren ] (7.75, 1.5);

    \node[] (times2) at (8.5, 0){$\times$};

    \node[] (annY) at (\posY, -1.8){\scalebox{0.85}{$Y \in \mathbb{R}^{n \times q}$}};
    \node[] (annX) at (\posX, -1.8){\scalebox{0.85}{$X \in \mathbb{R}^{n \times d}$}};
    \node[] (annE) at (\posE, -1.8){\scalebox{0.85}{$E \in \mathbb{D}^{d \times d}$}};
    \node[] (annN) at (\posN, -1.8){\scalebox{0.85}{$N \in \mathbb{R}^{n \times d}$}};
    \node[] (annF) at (\posF, -1.8){\scalebox{0.85}{$F \in \mathbb{R}^{d \times q}$}};

    \node[] (labY) at (\posY, 2){Observations};
    \node[] (labX) at (\posX, 2){Factors};
    \node[] (labE) at (\posE, 2.3){Cause};
    \node[] (labE) at (\posE, 2){selection};
    \node[] (labN) at (\posN, 2){Noise};
    \node[] (labF) at (\posF, 2.3){Cause-effect};
    \node[] (labF) at (\posF, 2){mapping};

    \node[] (sim1) at (0,-3.25) {\scalebox{0.85}{$X \sim P(X)$}};
    \node[] (sim2) at (0,-3.65) {\scalebox{0.85}{$N \sim P(N)$}};
    \node[] (reg1) at (5,-3.25) {$\hat{E} = \underbrace{(X_2^\top X_2 + \lambda_X)^{-1} X_2^\top Y_2\overbrace{(Y_1^\top Y_1 + \lambda_Y)^{-1} Y_1^\top X_1}^{\text{1) } \hat{X} : \text{ regression from } Y \text{ to } X}}_{\text{2) regression from } X \text{ to } \hat{X}} $};
    \end{tikzpicture}
    \caption{In its linear form, back-to-back regression identifies the subset of factors that suffices to account for multidimensional observations by piping two successive regressions from $Y$ to $X$ and from $X$ to $\hat X$. }
    \label{fig:}
\end{figure}

\subsection{Formalism}
Via an apparatus F, we are measuring a multidimensional signal Y, which is a function of possible causes X and other non causal features N. Not all features of X exert causal influence on Y. Let EX (E a matrix of zero and one) the subset of causal factors. Our model can be formalised as $Y=f(EX,N)$. 

Without loss of generality, we may assume N to be centred(adding, when necessary, an intercept to the causal features), and redefine features and noise so that the model be written $Y=f(EX+N)$ (f a function, E, X and N matrices). In particular, the addition of "external noise" M, as in $Y=F(EX+N)+M$ can be viewed as a small change in F and N.

We consider both the linear and non linear case. In the first one, our model is $$Y = F(EX+N)$$, with  $Y\in\mathbb{R}^{n\times ny}$, $F\in\mathbb{R}^{ny\times nx}$, $X\in\mathbb{R}^{nx\times n}$, $N\in\mathbb{R}^{nx\times n}$, and E a binary diagonal square matrice of size nx. 

In the non linear model, $F(EX+N)$ is defined as before, but Y is the result of a non linear transformation of it $$Y=\sigma (F(EX+N))$$

Under this formalism, we are given samples of Y and X, and are tasked to recover E and use it to predict Y from X.

\subsection{Algorithm}

Back-to-back regression consists of two steps.
%
First, we perform a least squares estimation of X from Y, that is we find $\hat G$ such that $\hat X=\hat G Y$ is the least square predictor of X.
%
This recovers an approximation of the causes of Y in X.
%
Then, we perform a least squares estimation of $\hat X$ from X, and determine $\hat H$ such that $\hat {\hat X}=\hat H X$ is the least square predictor of $\hat X$.
%
In matrix form, we have $\hat G=(Y'Y)^{-1} Y'X$, and
\begin{equation} \hat H=(X'X)^{-1} X'Y(Y'Y)^{-1} Y'X\end{equation}

If, in the above formalism, F is invertible with respect to the causal features of X (ie the problem has a solution), and the relation is linear or close to linear, the diagonal of $\hat H$ will be a scaled approximation of the diagonal of E. That is to say, back to back regression will recover the causal features of the process considered, even in the presence of heavy noise N, and without needing to estimate the transformation F.
 
In practical cases, the covariance matrices of Y and X will often be badly conditioned (because features of X are related, or measurement produces correlated features of Y). This makes the inversion of $X'X$ and $Y'Y$ sensitive to noise (either from N, from sampling or from computation rounding). To guard against high condition numbers, we have used two methods: truncated SVD (ref) and ridge regression (ref). 

In both, we compute the singular value decompositions of the covariance matrix, transform its diagonal part, and invert it. In truncated SVD, the lowest eigenvalues are set to zero, and the corresponding vectors eliminated from future calculations. In ridge regression, a constant factor is added to all eigenvalues. The regularisation factor and truncation threshold are chosen via leave one out cross validation. In practice, ridge regression tends to provide better stability but introduces a bias in the estimates. Truncated SVD is used to provide better results in low noise cases.

Performing two regressions over the same data sample can result in overfitting, as spurious correlations in the data are detected in the first step and learnt in the second. To avoid this, we split our sample into two halves, and each regression is performed over one, effectively decorrelating the two steps.
%
To compensate for the reduction in sample size caused by the split, this process is repeated over many splits, and their results are averaged.
This bagging compensates for the reduction in sample size caused by splitting.

As we shall see, the diagonal of $\hat H$ recovered by back to back regression is a scaled approximation of the diagonal of E. Noise tends reduce the large diagonal elements, whereas the bias introduced by regularisation tends to increase the lowest ones.

To recover an estimate of E, we need a test to select the largest diagonal elements of $\hat H$.
%
In regression analysis, a traditional approach to this problem consists in testing whether coefficients differ from zero, via variants of Student t-test.
%
This will not succeed here. First because the conditions for these tests do not apply here (we have two regressions). Second, because regularisation adds bias on the diagonal, which will cause t-tests to succeed unduly.
%
Instead, we treat feature extraction as a one dimensional binary clustering problem: classify n positive real values as "large" and "small" (under the hypothesis that they are not all small or all large). This can be done by maximizing the ratio of inter-group variance over total variance (ie minimizing intra-group variance), over all splits of the p largest data and the n-p smallest. 

If s and r are the average values of the two clusters, p and n-p their size and V the total variance of the sample, we select the split that maximises the Sonquist and Morgan criterion $$K = \frac{p (n-p)}{n} \frac{(s-r)^2}{ V}$$.
%
This recovers from $\hat H$ a binary diagonal $\hat E$.
%
To use back-to-back regression as a regression technique (to predict Y from X), we multiply X by $\hat E$ and perform a cross validated ridge regression of each feature of Y from $\hat E X$.

Summarizing, back-to-back regression is performed as follows:
\begin{enumerate}
\item Randomly split the data sample.
\item From the first half sample, using crossalidated ridge regression, find $\hat G$, the regularised least square regression of X from Y: $\hat G=(Y'Y)^{-1} Y'X$.
\item From the second half sample, calculate $\hat X = \hat G Y$, and find $\hat H$, the regularized least square regression of $\hat X$ from X: $\hat H=(X'X)^{-1} X'Y(Y'Y)^{-1} Y'X$.
\item Repeat steps 1 to 3 and average the results as $\hat H$.
\item Using the Sonquist and Morgan criterion, select the large diagonal elements of $\hat H$, set them to one and the rest of $\hat  H$ to zero.
\item Use the resulting matrix $\hat E$ as an estimator of E, the active features in the causal process
\item Alternatively, use $\hat E$ to predict Y from X, by performing a regularised least square regression of Y from $\hat E X$.
%

\end{enumerate}

\subsection{Asymptotic behaviour - linear case}
As discussed in the introduction, our problem can be formulated (up to a redefinition of the features of X) as $Y=f(EX+N)$, with f an unknown function.
%
If F is linear we can rewrite it as $Y = F(EX + N)$, with X an (dx, n) matrix of possible causes, E a (dx,dx) binary diagonal matrix selecting active features of X.
%
N a (dx,n) homoscedastic noise matrix, F an unknown (possibly full rank) (dy,dx) transformation corresponding to the measuring apparatus, and Y a (dy, n) matrix of measured effects.

The two steps of back-to-back regression amounts to finding $\hat G=\arg \min_G \left \| X-GY \right \|^2$ and $\hat H =\arg \min_H \left \| \hat GY - HX \right \|^2$ (all matrix norms henceforth are Frobenius norms).

For the first step, replacing Y, and since N is centred, we have
\begin{equation}
\begin{aligned}
\arg \min_G \left \| X-GY \right \|^2 &= \arg \min_G \left \| X - GF(EX+N)\right\|^2 \\
&{}= \arg \min_G \left \| X-GFEX\right\| ^2  + \left \| GFN\right \| ^2
\end{aligned}
\end{equation}

As N tends to zero, we are searching for the minimal norm solution of $\min_G \left \| X-GFEX\right\| ^2$, which is $X(FEX)^\dagger$ ($M^\dagger$ denoting the pseudo inverse of M).

Let k be the number of active features of E, suppose for notational convenience that they are the first k features of X, and let $FEX$ be of rank k (which means F and X have full rank on the space spanned by the active features of E). Let K be the $\mathbb{R}^{nx\times k}$ diagonal matrix obtained by deleting the rightmost nx-k columns of E. Then, $E=KK'$, and $FEX=(FK)(K'X)$, with $FK$  and $K'X$ of full rank k.

The rank factorisation property of the pseudo inverse (ref) says that, if $A=BC$ with $A\in\mathbb{R}^{m\times n}$, $B\in\mathbb{R}^{m\times k}$ and $C\in\mathbb{R}^{k\times n}$ of rank k, then $A^\dagger=C^\dagger B^\dagger$ and since C is full rank, $C^\dagger=C' (CC')^{-1}$.

Replacing in the expression of the minimal norm solution yields
\begin{equation}
\begin{aligned}
X(FEX)^\dagger &=X((FK)(K'X))^\dagger \\
&=X(K'X)'((K'X)(K'X)')^{-1}(FK)^\dagger \\
&=(XX')K(K'XX'K)^{-1}(FK)^\dagger 
\end{aligned}
\end{equation}

Let $XX' = \left(\begin{array}{c|c}\Sigma_{1} & \Sigma_{2} \\\hline \Sigma_{2} & \Sigma_{3}\end{array}\right)$, the top left block being $k\times k$. Then, $(K'XX'K)=\Sigma_{1}$ and $(XX')K(K'XX'K)^{-1}=\left(\begin{array}{c}I_{k} \\\hline \Sigma_{2} \Sigma_{1}^{-1}\end{array}\right)$, call it S.

Thus, in the absence of noise, and if F and X have full rank on the subspace spanned by E, we have $G\hat=X(FEX)^\dagger =S (FK)^\dagger$. If the covariance of X is block diagonal, that is if inactive features and active features are not correlated (if the causal features do not serve as confounders for non causal ones), then S=K, and we are retrieving $K (FK)^\dagger$.

Under centred noise, and if the covariance of FN has low condition number (a weak assumption given that N is noise), we have $\left \| GFN\right \| ^2 \approx Var(FN) \left \| G\right \| ^2$, and minimisation of $\left \| X-GFEX\right\| ^2  + \left \| GFN\right \| ^2$ will recover a scaled version of the noiseless solution, $\lambda S (FE)^\dagger$, $\lambda$ a positive number below one.

Replacing in the left part of (ref), we have $X-GFEX=X- \lambda S (FK)^\dagger FKK'X= X - \lambda EX=X(I-\lambda E)$. Its square norm is $(1-\lambda)^2 Var(X_{active}) + Var(X_{inactive})$ ($Var(X_{.})$ denoting the variance of X over active and inactive features).

For the right part, we have $GFN=\lambda S (FK)^\dagger FN= \lambda S (FK)^\dagger (FK+F(I-K))K'N= \lambda S EN=  \lambda EN$. (Since F is full rank over E, we have $(FK)^\dagger F(I-K)= 0$). The square norm is therefore $\lambda^2 Var(N_{active})$.

Thus, with centred and random noise, and with F full rank over E, we are minimising $(1-\lambda)^2 Var(X_{active}) + Var(X_{inactive})+\lambda^2 Var(N_{active})$. Its least value is attained for $\lambda = \frac{Var(X_{active})}{Var(X_{active})+Var(N_{active})}$. Assuming X and N have the same variance over all features (this can always be guaranteed), active subscript can be dropped, and this simplifies to $\lambda= \frac {Var(X)}{Var(X)+ Var(N)}=\frac{1}{1+nsr}$ nsr the noise to signal ratio, Var(N) divided by Var(X).

The first step of the back to back regression therefore retrieves $\frac{1}{1+nsr} S (FK)^\dagger$.

The second step is straightforward, replacing Y, we have
\begin{equation}
\begin{aligned}
\arg \min_H \left \| \hat GY - HX \right \|^2 &=\arg  \min_H \left \| \hat GF(EX+N) - HX \right \|^2 \\
&=\arg \min_H \left \| (\hat G(FE)-H)X \right \| ^2 + \left \| \hat GFN \right \| ^2\\
&= \arg \min_H \left \| (\hat G(FE)-H)X \right \| ^2\\
&=\hat G (FE)
\end{aligned}
\end{equation}

Thus, in the linear case, back to back regression retrieves $\hat H = \frac{1}{1+nsr} S (FK)^{\dagger}(FKK') = \frac{1}{1+nsr} E$.

\subsection{Asymptotic behaviour - non linear case}

\section{Related works}
\subsection{back-to-back regression as a special case of canonical component analysis?}
Formula (1) shows that back-to-back regression may be related to canonical component analysis (CCA).
%
In CCA, we are given two matrices X and Y (or size N,p and N,q) that describe different features measured on the same data sample, and characterize their proximity (that is, between the subspaces spanned by their columns) as the correlation between linear combinations of X and Y.

This is done by finding vectors $a$ and $b$ such that $Xa$ and $Yb$ display maximal correlation, or by maximizing $a'X'Yb$ over unit-normed $a$ and $b$.
%
To find a, one calculates the eigenvectors of Eq. (1).
%
The largest one is the direction with maximal correlation, the second one the maximal correlation direction orthogonal to the first, and so on.
%
For each canonical dimension, the corresponding eigenvalue is the square of the cosine between the corresponding direction along X and the subspace spanned by Y.

If the result of Eq. (1) is diagonal, as in the model presented above, the features of X are eigenvectors.
%
If all eigenvalues are either 0 or 1, then all the features are either part of the Y subspace (eigenvalues of one) or orthogonal to it (for zero eigenvalues) because they correspond to the squared cosine of the angle between the corresponding feature and the subspace spanned by Y, .
%
Thus, back-to-back regression amounts to CCA under the constraint that the subspaces spanned by X and Y are perpendicular, X is spanned by vectors of Y and vectors orthogonal to Y, and that the parallel and orthogonal components of X are spanned by disjoint sets of features of X.
%
The closer we are to this hypothesis, the closer the results of CCA and back-to-back regression will be.

There are differences between CCA and back-to-back regression.
%
First, since our objective is not to characterise the proximity between X and Y, but just to extract causal features, we can dispense with the last step of CCA (diagonalisation of (1)).
%
Second, since we operate in a noisy environment, we use regularisation, introducing bias in the correlation calculations.
%
Finally, whereas CCA wants to leverage all correlations between X and Y, we filter some of them, through splitting and bagging.
%
In other words, whereas we are working from the same formulae as CCA, we use them in different ways, for different purposes

\subsection{Related regression methods}
FC: discuss similar methods of regression, that can be used for feature extraction
And maybe have something about other causal discovery methods

\section{Experiments - simulated data}
\input{synthetic.tex}

\section{Experiments - real case}
Jean Remi and Maxime territory

\input{meg_exps.tex}

\section{Conclusion}

\section{Acknowlegements}
Data were provided (in part) by the Donders Institute for Brain, Cognition and Behaviour.

\clearpage
\newpage

\bibliographystyle{abbrvnat}
\bibliography{paper}

\section{Appendices}

This should hold part of the tests, explanations on simulation, detailed results and stuff on meg data

\end{document}
