\documentclass[preprint,12pt,3p]{elsarticle}

\usepackage{lineno,hyperref}
% \modulolinenumbers[5]

\usepackage{amsmath, amsfonts, microtype, xcolor, tikz, graphicx, amsthm}
\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage{times,wrapfig}
\usepackage{cleveref}


\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}

\SetKwComment{Comment}{$\triangleright$\ }{}

\usetikzlibrary{calc}

\tikzset{
    ncbar angle/.initial=90,
    ncbar/.style={
        to path=(\tikztostart)
        -- ($(\tikztostart)!#1!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztotarget)$)
        -- ($(\tikztotarget)!($(\tikztostart)!#1!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztotarget)$)!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztostart)$)
        -- (\tikztotarget)
    },
    ncbar/.default=0.5cm,
}

\tikzset{round left paren/.style={ncbar=0.5cm,out=110,in=-110}}
\tikzset{round right paren/.style={ncbar=0.5cm,out=70,in=-70}}


% \journal{Neuroimage}

% \bibliographystyle{elsarticle-num}
\bibliographystyle{plain}

\begin{document}
\begin{frontmatter}

\title{Discriminating the Influence of Correlated Factors \\from Multivariate Observations: the Back-to-Back Regression}

\author[1,2]{Jean-Rémi King\corref{email}}
\cortext[email]{corresponding author: jeanremi@fb.com}

\author[2]{Fran\c{c}ois Charton}
\author[2]{David Lopez-Paz}
\author[2]{Maxime Oquab}
\address[1]{Laboratoire des systèmes perceptifs, PSL University, CNRS}
\address[2]{Facebook AI}


\begin{abstract}
Identifying causes solely from observations can be particularly challenging when
i) potential factors are difficult to manipulate independently and ii)
observations are multi-dimensional. To address this issue, we
introduce ``Back-to-Back'' regression (B2B), a linear method designed to efficiently
measure, from a set of correlated factors, those that most plausibly account for
multidimensional observations. First, we prove the consistency of B2B, its links to
other linear approaches, and show how it provides a robust, unbiased and
interpretable scalar estimate for each factor.
Second, we use a variety of simulated data to show that B2B outperforms
least-squares regression and cross-decomposition techniques (e.g. canonical
correlation analysis and partial least squares) on causal identification when
the factors and the observations are partially collinear.
Finally, we apply B2B to magneto-encephalography and functional Magnetic
Resonance Imaging of 102 subjects
recorded during a one hour long reading task to test whether our method appropriately disentangles
the respective contribution of collinear factors such as word length, word
frequency respectively known to cause early visual and late associative cortical
responses. The results
show that these factors are better disentangled with B2B than with
other standard techniques.
\end{abstract}


\begin{keyword}
  Cross-Decomposition \sep
  Feature Discovery \sep
  MEG \sep
  fMRI \sep
  Decoding \sep
  Encoding \sep
  Reading \sep
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}
\input{introduction.tex}

\section{Back-to-Back regression}
\label{sec:algorithm}

\begin{figure}[t!]

  \centering
  \begin{tikzpicture}
  \newcommand\posY{0}
  \newcommand\posX{3}
  \newcommand\posS{5}
  \newcommand\posN{7}
  \newcommand\posF{10}

  \node[thick, draw=black, minimum height=3cm, minimum width=2cm, fill=yellow!10] (Y) at (\posY, 0){};
  \node[] (eq) at (1.5, 0){$=$};
  \node[] (times) at (4, 0){$\times$};
  \node[thick, draw=black, minimum height=3cm, minimum width=1cm, fill=yellow!10] (X) at (\posX, 0){};
  \node[thick, draw=black, minimum height=1cm, minimum width=1cm, fill=red!10] (S) at (\posS, 0){};

  \draw[fill=white] (\posS - 0.5 + 0.0, 0.5 - 0.0) rectangle (\posS - 0.5 + 0.0 + 0.1, 0.5 - 0.0 - 0.1);
  \draw[fill=black] (\posS - 0.5 + 0.1, 0.5 - 0.1) rectangle (\posS - 0.5 + 0.1 + 0.1, 0.5 - 0.1 - 0.1);
  \draw[fill=white] (\posS - 0.5 + 0.2, 0.5 - 0.2) rectangle (\posS - 0.5 + 0.2 + 0.1, 0.5 - 0.2 - 0.1);
  \draw[fill=white] (\posS - 0.5 + 0.3, 0.5 - 0.3) rectangle (\posS - 0.5 + 0.3 + 0.1, 0.5 - 0.3 - 0.1);
  \draw[fill=white] (\posS - 0.5 + 0.4, 0.5 - 0.4) rectangle (\posS - 0.5 + 0.4 + 0.1, 0.5 - 0.4 - 0.1);
  \draw[fill=white] (\posS - 0.5 + 0.5, 0.5 - 0.5) rectangle (\posS - 0.5 + 0.5 + 0.1, 0.5 - 0.5 - 0.1);
  \draw[fill=white] (\posS - 0.5 + 0.6, 0.5 - 0.6) rectangle (\posS - 0.5 + 0.6 + 0.1, 0.5 - 0.6 - 0.1);
  \draw[fill=black] (\posS - 0.5 + 0.7, 0.5 - 0.7) rectangle (\posS - 0.5 + 0.7 + 0.1, 0.5 - 0.7 - 0.1);
  \draw[fill=white] (\posS - 0.5 + 0.8, 0.5 - 0.8) rectangle (\posS - 0.5 + 0.8 + 0.1, 0.5 - 0.8 - 0.1);
  \draw[fill=black] (\posS - 0.5 + 0.9, 0.5 - 0.9) rectangle (\posS - 0.5 + 0.9 + 0.1, 0.5 - 0.9 - 0.1);

  \node[] (plus) at (6, 0){$+$};
  \node[thick, draw=black, minimum height=3cm, minimum width=1cm, fill=blue!10] (N) at (\posN, 0){};
  \node[thick, draw=black, minimum height=1cm, minimum width=2cm, fill=red!10] (F) at (\posF, 0){};
  \draw [thick] (2.25, -1.5) to [round left paren ] (2.25, 1.5);
  \draw [thick] (7.75, -1.5) to [round right paren ] (7.75, 1.5);

  \node[] (times2) at (8.5, 0){$\times$};

  \node[] (annY) at (\posY, -1.8){\scalebox{0.85}{$Y \in \mathbb{R}^{m \times d_y}$}};
  \node[] (annX) at (\posX, -1.8){\scalebox{0.85}{$X \in \mathbb{R}^{m \times d_x}$}};
  \node[] (annS) at (\posS, -1.8){\scalebox{0.85}{$S \in \mathbb{D}^{d_x \times d_x}$}};
  \node[] (annN) at (\posN, -1.8){\scalebox{0.85}{$N \in \mathbb{R}^{m \times d_x}$}};
  \node[] (annF) at (\posF, -1.8){\scalebox{0.85}{$F \in \mathbb{R}^{d_x \times d_y}$}};

  \node[] (labY) at (\posY, 2){Observations};
  \node[] (labX) at (\posX, 2){Factors};
  \node[] (labS) at (\posS, 2){Cause};
  \node[] (labS) at (\posS, 1.7){selection};
  \node[] (labN) at (\posN, 2){Noise};
  \node[] (labF) at (\posF, 2){Cause-effect};
  \node[] (labF) at (\posF, 1.7){mapping};

  \node[] (sim1) at (0,-3.25) {\scalebox{0.85}{$X \sim P(X)$}};
  \node[] (sim2) at (0,-3.65) {\scalebox{0.85}{$N \sim P(N)$}};
  \node[] (reg1) at (6,-3.25) {$\hat{S} = \text{diag}(\underbrace{(X_2^\top X_2 + \Lambda_X)^{-1} X_2^\top Y_2\overbrace{(Y_1^\top Y_1 + \Lambda_Y)^{-1} Y_1^\top X_1}^{\text{1) } \hat{X} : \text{ regression from } Y \text{ to } X}}_{\text{2) regression from } X \text{ to } \hat{X}})$};
  \end{tikzpicture}
  \caption{Back-to-back regression identifies the subset of factors $S_{ii} = 1$ in $X$ that influence some observations $Y$ by 1) regressing from $Y$ to $X$ to obtain $\hat{X}$, and 2) returning the diagonal of the regression coefficients from $X$ to $\hat{X}$.}
\label{fig:b2b}
\end{figure}

\subsection{Problem setup}

We consider the measurement of multivariate signal $Y \in \mathbb{R}^{m \times
d_y}$ (the dependent variables), generated from a set of putative causes $X \in \mathbb{R}^{m \times
d_x}$ (the independent variables), via some unknown linear apparatus $F \in \mathbb{R}^{d_x \times d_y}$.
%
Not all the variables in $X$ exert a causal influence on $Y$.
%
By considering a square binary diagonal matrix of \emph{causal influences} $S
\in \mathbb{D}^{d_x \times d_x}$, we denote by $XS$ the causal factors of $Y$.
%
In summary, the problem can be formalized as:
%
\begin{equation}
    y_i = (x_i S + n_i) F
    \label{eq:model}
\end{equation}
%
where $i$ is a given sample, and $n_i$ is a sample-specific additive noise drawn from a
centered distribution.
%
While the triplet of variables $X$ and $N$ are independent, we allow each of
them to have any form of covariance.
% what is a general covariance matrix?
%
In practice, we observe $m$ samples $(X, Y)$ from the model.
%
This problem space, along with the sizes of all variables involved, is
illustrated in Figure~\ref{fig:b2b}.
%
Given the model in Equation~\cref{eq:model}, the goal of Back-to-Back
Regression (B2B) is to estimate the matrix $S$, i.e. to identify the factors
that most reliably account for the multivariate observations.

\subsection{Algorithm}

Back-to-Back Regression (B2B) consists of two steps.
%
First, we estimate the linear regression coefficients $\hat G$ from $Y$ to $X$,
and construct the predictions $\hat X = Y \hat G$.
%
This backward regression recovers the correlations between $Y$ and each factor
of $X$.
%
Second, we estimate the linear regression coefficients $\hat H$ from $X$ to
$\hat X$.
%
The diagonal of the regression coefficients $\hat H$, denoted by $\hat{S} =
\text{diag}(\hat{H})$, is the desired estimate of the causal influence matrix
$S$, as detailed in the \ref{appendix:theorem_proof}.

If using l2-regularized least-squares \citep{hoerl1959optimum, rifkin2007notes},
B2B has a closed form solution:
\begin{align}
    \hat G &= (Y^\top Y + \Lambda_Y)^{-1} Y^\top X,\label{eq:solG}\\
    \hat H &=(X^\top X + \Lambda_X)^{-1} X^\top Y \hat G,\label{eq:solH}
\end{align}
%
where $\Lambda_X$ and $\Lambda_Y$ are two diagonal matrices of regularization
parameters, useful to invert the covariance matrices of $X$ and $Y$ if these are
ill-conditioned.

Performing two regressions over the same data sample can result in overfitting,
as spurious correlations in the data absorbed by the first regression will be
leveraged by the second one.
%
To avoid this issue, we split our sample $(X, Y)$ into two splits $(X_1, Y_1)$
and $(X_2, Y_2)$.
%
The first regression is performed using $(X_1, Y_1)$, and the second
regression is performed using $(X_2, Y_2)$.
%
To compensate for the reduction in sample size caused by the split, the two
successive regressions are
repeated over many random splits, and the final estimate $\hat S$ of the causal
influence matrix is the average over the estimates associated to each split
\citep{breiman1996bagging}.
%
To accelerate this ensembling procedure, we use an efficient
leave-one-out cross-validation scheme as detailed in \citep{rifkin2007notes}
% http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf,
as follows:
%

\begin{equation}
\hat{Y} = (\Sigma_X G Y - \text{diag}(\Sigma_X G) Y) \;/\; \text{diag}(I - \Sigma_X G) \qquad \text{(element-wise division)}
\end{equation}

where $\Sigma_X$ is the $X$ kernel matrix and where $G$ is computed with an
eigen decomposition of $X$:
\begin{equation}
  \begin{aligned}
  \Sigma_X = Q V Q^T \\
  G = Q (V + \lambda I)^{-1} Q^T\\
  \end{aligned}
\end{equation}
where $Q$, $V$ and $\lambda$ are the eigen vectors, eigen values and
regularization, respectively.

%
% After obtaining $\hat{S}$, we can fit a final regression from $X \hat{S}$ to $Y$.

We summarize the B2B procedure in Algorithm~\ref{algorithm:b2br}.
%
The rest of this section provides a theoretical guarantee on the correctness of
B2B.


% JR: shall we update algorithm with LOO trick?
\begin{algorithm}[H]
    %\SetAlgoLined
    \KwIn{input data $X \in \mathbb{R}^{m \times d_x}$, output data $Y \in \mathbb{R}^{m\times d_y}$, number of repetitions $m \in \mathbb{N}$.}
    \KwOut{estimate of causal influences $\hat{S} \in \mathbb{D}^{d_x \times d_x}$.}
    $\hat{S} \leftarrow 0$\;
    \For{$i = 1, \ldots, m$}{
        $(X, Y) \leftarrow \text{ShuffleRows}((X, Y))$\;
        $(X_1, Y_1), (X_2, Y_2) \leftarrow \text{SplitRowsInHalf}((X, Y))$\;
        $\hat{G} = \text{LinearRegression}(Y_1, X_1)$ \Comment*[r]{$\hat G = (Y_1^\top Y_1 + \Lambda_Y)^{-1} Y_1^\top X_1$}
        $\hat{H} = \text{LinearRegression}(X_2, Y_2 \hat{G})$ \Comment*[r]{$\hat H=(X_2^\top X_2 + \Lambda_X)^{-1} X_2^\top Y_2 \hat G$}
        $\hat{S} \leftarrow \hat{S} + \text{diag}(\hat{H})$\;
    }
    $\hat{S} \leftarrow \hat{S} / m$\;
    $\hat{W} \leftarrow \text{LinearRegression}(X \hat{S}, Y)$\;
    \Return{$\hat{S}$, $\hat{W}$}
    \caption{Back-to-back regression.}
    \label{algorithm:b2br}
\end{algorithm}

\subsection{Theoretical guarantees}
\label{sec:theorem}

\begin{theorem}[B2B consistency - general case]

     Consider the B2B model from Equation $Y = (XS + N)F$, $N$ centered and full
     rank noise.
     %
     Let $Img(M)$ refers to the image of the matrix $M$. If $F$ and $X$ are
     full-rank on the $Img(S)$, then, the solution of B2B, $\hat H$, will
     minimize
     %
     $\min_H  \left \| X - XH\right\| ^2  + \left \| NH\right \| ^2$and satisfy $S\hat H = \hat H$
\end{theorem}
%
\begin{proof}
  See Appendix \ref{appendix:theorem_proof}.
\end{proof}

Since  $S\hat H = \hat H$, we have
\begin{equation}
  \hat H = \arg \min_H  \left \| X - XSH\right\| ^2  + \left \| NSH\right \| ^2 = (S X^\top XS +SN^\top NS) ^\dagger SXX^\top.
\end{equation}
% So, $\hat H = \arg \min_H  \left \| X - XSH\right\| ^2  + \left \| NSH\right \| ^2 = (S X^\top XS +SN^\top NS) ^\dagger SXX^\top$.

Assuming, without loss of generality, that the active features in $S$ are the $k
\in \mathbb{Z}: k \in [0, d_x]$ first features, and rewriting $X=(X_1,X_2)$ and
$N=(N_1,N_2)$ ($X_1$ and $N_1$ containing the $k$ first features), we have:

% $$
\begin{equation}
  X^\top X = \left(\begin{array}{lccl}\Sigma_{X_1 X_1} & \Sigma_{X_1 X_2} \\ \Sigma_{X_1 X_2} & \Sigma_{X_2 X_2}\end{array}\right),\qquad\qquad N^\top N = \left(\begin{array}{lccl}\Sigma_{N_1 N_1} & \Sigma_{N_1 N_2} \\ \Sigma_{N_1 N_2} & \Sigma_{N_2 N_2}\end{array}\right),
\end{equation}

where $\Sigma_{A B}$ is the covariance of $A$ and $B$, and:
% and
\begin{equation}
  \hat H = \left(\begin{array}{cc}(\Sigma_{X_1 X_1}+\Sigma_{N_1 N_1})^{-1}\Sigma_{X_1 X_1} & (\Sigma_{X_1 X_1}+\Sigma_{N_1 N_1})^{-1}\Sigma_{X_1 X_2} \\0 & 0\end{array}\right)
\end{equation}
\begin{equation}
  \text{diag}_k (\hat H) = \text{diag}((\Sigma_{X_1 X_1}+\Sigma_{N_1 N_1})^{-1}\Sigma_{X_1 X_1}) = \text{diag}((I+\Sigma_{X_1 X_1}^{-1}\Sigma_{N_1 N_1})^{-1})
  \label{eq:diagk}
\end{equation}
%
% $$
% $$
% $$
% $$
%
In the absence of noise, we have $\Sigma_{N_1 N_1}=0$, and so
$\text{diag}_k(\hat H)=I$, and $$\text{diag}(\hat H) = \text{diag}(S)$$
Therefore, we recover $S$ from $\hat H$.


In the presence of additive noise, the causal factors of $S$ correspond to the positive
elements of $\text{diag}(\hat H)$. The methods to recover them are presented in
the Appendix (\ref{recovering}).

\section{Experiments}

We perform two sets of experiments to evaluate B2B: one on controlled synthetic
data, and a second one on a real, large-scale magneto-encephalography (MEG)
dataset.
%
We use scikit-learn's PLS and RidgeCV \citep{sklearn} as well as Pyrcca's
regularized canonical component analysis (RegCCA, \citep{bilenko2016pyrcca})
objects to compare B2B against the standard baselines.

\subsection{Synthetic data}
\label{sec:experiment_synthetic}
\input{synthetic.tex}

\subsection{functional Magnetic Resonance Imaging}
\label{sec:experiment_fmri}
\input{fmri_exps.tex}

\subsection{Magneto-encephalography}
\label{sec:experiment_real}
\input{meg_exps.tex}

\section{Related work}

% Forward and cross-decomposition models have been used to identify the causal
% contribution of correlated factors onto multi-dimensional observations (e.g.
% \citep{naselaris2011encoding}). These approaches typically lead to multiple
% coefficients for each features (i.e. one per dimension of $Y$ or one per
% component respectively). Furthermore, these coefficients can be difficult to
% summarize into a single causal estimate. By contrast, B2B quickly (Fig.
% \ref{fig:duration}) leads to a single unbiased scalar values $\hat S$ tending
% towards 1 and 0 for causal and non-causal features respectively.

A variety of statistical methods applied to neuroimaging have been
proposed to clarify what is being represented in brain responses - i.e. what
feature causes specific brain activity. For example, CCA and PLS have been used in
neuroimaging for a variety of purposes such as
denoising and subject alignment \citep{cca_hotelling, de2019multiway}. While CCA
and PLS relates to B2B, these methods diverge in several ways. First, they have different objectives: CCA aims to find the potentially numerous and poorly
interpretable components where $X$ and $Y$ are maximally correlated, whereas B2B
aims to recover the causal factors from X to Y. Second, B2B is not symmetric
between $X$ and $Y$: it aims to identify specific causal features by first
optimizing over the decoders $G$ and then over $H$. By contrast, CCA and PLS are
symmetric between $X$ and $Y$, and aims to find $G$ and $H$ such that they
project $X$ and $Y$ on maximally correlated dimensions. Third, CCA is based an
eigen decomposition of $XH$ and $YG$ - the corresponding canonical components
are thus mixing the $X$ features in way that limit interpretability and
potentially dilute the impact of each feature onto multiple components. In
contrast B2B assesses each feature $X^j$ on a single $Y$ component specifically
selected to maximize signal-to-noise ratio of that feature $j$. Fourth CCA does
not separately optimize two distinct regularization
parameters for $G$ and $H$, where B2B does. Finally, CCA does not use different data splits to
estimate $G$ and $H$. Together, these differences may explain why B2B reliably
outperform CCA on estimating causal influences (Figs. \ref{fig:percondition} and
\ref{fig:auc_plots}).

One popular multivariate analysis of brain activity is
Representational Similarity Analysis (RSA)
\citep{kriegeskorte2008representational}, and consists in analyzing the
similarity of brain responses associated with specific categorical conditions
(e.g. distinct images), by (1) fitting one-against-all classifiers on each
condition and (2) testing whether these classifiers can discriminate all other
conditions. The resulting confusion matrix is then analyzed, generally in an unsupervised
manner, to reveal which conditions lead to similar brain activity patterns. B2B
differs from RSA in that it uses regressions instead of one-hot classifications.
Consequently, and unlike RSA, B2B (1) is fully supervised (2) provides
interpretable coefficients and (3) can generalize to new items and new contexts.
In practice, these elements allow B2B to apply to event-related paradigms and
latent variable analyses.

The present empirical results, together with their theoretical foundations,
suggest that B2B may come as a tool of choice for causal discovery when
feature-wise intervention is limited.


\section{Acknowledgements}
We are thankful to Gael Varoquaux and Alexandre Gramfort for the valuable
feedback. This work was supported by ANR-17-EURE-0017 and the Fyssen Foundation
to JRK.


\bibliography{paper}

\newpage
\clearpage
\section{Appendices}
\input{appendix}

\end{document}
