\documentclass{article}

\usepackage[final]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}

\title{Learning causal features with double regression}

\author{%
  Jean-Remi King\\
  Facebook AI\\
  \texttt{email} \\
  \And
  Fran\c{c}ois Charton\\
  Facebook AI\\
  \texttt{fcharton@fb.com}\\
  \And
  David Lopez-Paz\\
  Facebook AI\\
  \texttt{dlp@fb.com}
}

\begin{document}

\maketitle

\begin{abstract}
    To be written.
\end{abstract}

\section{Introduction}


A survey in deep learning \citep{deep_learning_nature}. 

\subsection{Contributions}

\section{Math corner}

\subsection{Problem formulation, asymptotic estimates}

Our problem may be formulated as
\begin{equation}
    Y = F(EX + N)
    \label{eq:model}
\end{equation}
where F is an unknown matrix, presumably full or high rank, corresponding to the measurement apparatus, E is a low rank transformation carried on the features (X), and N a noise term, centred but not necessarily gaussian or diagonal.

The first regression (X from Y), amounts to finding
\begin{equation}
\begin{aligned}
\min_G \left \| X-GY \right \|^2 &= \min_G \left \| X - GF(EX+N)\right\|^2 \\
&{}= \min_G \left \| (I-GFE)X\right\| ^2 + \left \| GFN\right \| ^2\\
&{}\leq \min_G \left \| I-GFE\right\| ^2 \left \| X\right\| ^2 + k\left \| G\right \| ^2\\ 
\end{aligned}
\end{equation}
since N is centred, and with k a positive constant which depends on the variance of FN.

This amounts to finding the minimal norm generalized inverse of FE, or its Moore-Penrose inverse
\begin{equation}
\hat  G= (FE)^{\dagger}
\end{equation}

The second regression is from X to the predicted value of X in the above, or $\hat G  Y$

\begin{equation}
\begin{aligned}
\min_H \left \| (FE)^{\dagger}Y - HX \right \|^2 &= \min_H \left \| (FE)^{\dagger}F(EX+N) - HX \right \|^2 \\
&= \min_H \left \| ((FE)^{\dagger}(FE)-H)X \right \| ^2 + \left \| (FE)^{\dagger}FN \right \| ^2\\
&= \min_H \left \| ((FE)^{\dagger}(FE)-H)X \right \| ^2\\
\end{aligned}
\end{equation}

Therefore, under the hypothesis that N is centred, our regression method estimates 
\begin{equation}
\begin{aligned}
\hat H =(FE)^{\dagger}(FE)
\end{aligned}
\end{equation}

For any matrix A, $A^\dagger A$ is the orthogonal projector over $Img(A')$, and $I-A^\dagger A$ the orthogonal projector over $Ker(A)$. 

If E selects some features of X (ie $Ex=0$ on any unselected feature, and E full ranked over the set of selected features) and if F is full rank over the subspace spanned by the features selected by E, then 
\begin{equation}
\begin{aligned}
Ker(FE) = Ker(E)  
\end{aligned}
\end{equation}
$I - \hat H$ is the projector over $Ker(E)$, the diagonal matrix that spans the unselected features, and $\hat H$ is the diagonal matrix corresponding to the selected features.

Under such hypotheses, $\hat H$ retrieves the features of X that have a causal influence on Y, and should do it in a fairly resilient way, since it is diagonal, and all its eigenvalues (0 or 1) are well separated.

\subsection{The double regression estimator, and canonical component analysis}

For any real matrices of X and Y, the least square regression of Y from X is calculated by the formula (X' denoting the transpose of X)
\begin{equation}
\begin{aligned}
(X'X)^{-1} X'Y
\end{aligned}
\end{equation}
and the prediction of Y thus obtained (from X) can be written
\begin{equation}
\begin{aligned}
X(X'X)^{-1} X'Y
\end{aligned}
\end{equation}
 
In double regression, we first calculate the regression of X from Y, and calculate $\hat X$ the corresponding prediction as $Y(Y'Y)^{-1} Y'X$. The regression of $\hat X$ from X then becomes
\begin{equation}
\begin{aligned}
(X'X)^{-1} X'Y(Y'Y)^{-1} Y'X
\end{aligned}
\end{equation}
Under ideal conditions (good conditioning of $X'X$ and $Y'Y$), this should recover a diagonal binary matrix, the 1 on the diagonal corresponding the the features of X used by Y. In a noisy environment, double regression tries to extract the useful features of X (ie those having an influence on Y) as non zero elements of the diagonal of (9).


(here be statistical test for the diagonal, and rationale for it)



Formula (9) shows that double regression can be understood as a special case of canonical component analysis (CCA). In CCA, given two matrixes (n,p and n,q) X and Y describing different features measured on the same data sample, one tries to characterise the closeness between X and Y (ie between the subspaces spanned by their columns) as the correlation between linear combinations of X and Y. This is done by finding vectors $a$ and $b$ such that $Xa$ and $Yb$ display maximal correlation, or by maximising $a'X'Yb$ over unit-normed $a$ and $b$. To find a, one calculates the eigenvectors of (9). The largest one is the direction with maximal correlation, the second one the maximal correlation direction orthogonal to the first, and so on. For each canonical dimension, the corresponding eigenvalue is the square of the cosine between the corresponding direction along X and the subspace spanned by Y.  

If (9) is diagonal, for instance in the model presented above, the features of X are eigenvectors. If all eigenvalues are either 0 or 1, since they correspond to the squared cosine of the angle between the corresponding feature and the subspace spanned by Y, all the features are either part of the Y subspace (eigenvalues of one) or orthogonal to it (for zero eigenvalues). 

Thus, double regression amounts to CCA under the constraint that the subspaces spanned by X and Y are perpendicular, X is spanned by vectors of Y and vectors orthogonal to Y, and that the parallel and orthogonal components of X are spanned by disjoint sets of features of X. The closer we are to this hypothesis, the closer the results of CCA and double regression will be.

\subsection{Stability analysis}

\subsection{Related approaches}
(PLS, 2 way, 3 fold...)

\section{Experiments}

\section{Conclusion}

\clearpage
\newpage

\bibliographystyle{abbrvnat}
\bibliography{paper}

\end{document}
