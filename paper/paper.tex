\documentclass[preprint,12pt,3p]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\usepackage{amsmath, amsfonts, microtype, xcolor, tikz, graphicx, amsthm}
\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage{times,wrapfig}
\usepackage{cleveref}


\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}

\SetKwComment{Comment}{$\triangleright$\ }{}

\usetikzlibrary{calc}

\tikzset{
    ncbar angle/.initial=90,
    ncbar/.style={
        to path=(\tikztostart)
        -- ($(\tikztostart)!#1!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztotarget)$)
        -- ($(\tikztotarget)!($(\tikztostart)!#1!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztotarget)$)!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztostart)$)
        -- (\tikztotarget)
    },
    ncbar/.default=0.5cm,
}

\tikzset{round left paren/.style={ncbar=0.5cm,out=110,in=-110}}
\tikzset{round right paren/.style={ncbar=0.5cm,out=70,in=-70}}


\journal{Neuroimage}

% \bibliographystyle{elsarticle-num}
\bibliographystyle{plain}

\begin{document}
\begin{frontmatter}

\title{Discriminating the Linear Influences of Collinear Factors \\from Multivariate Observations: the Back-to-Back Regression}

\author[1,2]{Jean-Rémi King\corref{email}}
\cortext[email]{corresponding author: jeanremi@fb.com}

\author[2]{Fran\c{c}ois Charton}
\author[2]{David Lopez-Paz}
\author[2]{Maxime Oquab}
\address[1]{Laboratoire des systèmes perceptifs, PSL University, CNRS}
\address[2]{Facebook AI}


\begin{abstract}
Identifying causes solely from observations can be particularly challenging when
i) potential factors are difficult to manipulate individually and ii)
observations are multi-dimensional. To address this issue, we
introduce ``Back-to-Back'' regression (B2B), a linear method designed to efficiently
measure, from a set of collinear factors, those that most plausibly account for
multidimensional observations. First, we prove the consistency of B2B, its links to
other linear approaches, and show how it provides a robust, unbiased and
interpretable scalar estimate for each factor.
Second, we use a variety of simulated data to show that B2B outperforms
least-squares regression and cross-decomposition techniques (e.g. canonical
correlation analysis and partial least squares) on causal identification when
both the factors and the observations are collinear.
Finally, we apply B2B to magneto-encephalography of 102 subjects
recorded during a reading task to test whether our method appropriately disentangles
the respective contribution of word length and word frequency - two collinear
factors known to cause early and late brain responses respectively. The results
show that these two factors are better disentangled with B2B than with
other standard techniques. We finally discuss the limits of B2B.
\end{abstract}


\begin{keyword}
  Cross-Decomposition \sep
  Feature Discovery \sep
  Magnetoencephalography \sep
  Decoding \sep
  Encoding \sep
  Reading \sep
  N400
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}

\input{introduction.tex}

\section{Back-to-Back regression}
\label{sec:algorithm}

\begin{figure}[t!]
  \include{fig_problem.tex}
  \label{fig:b2b}
\end{figure}

We consider the measurement of multivariate signal $Y \in \mathbb{R}^{n \times
d_y}$ (the dependent variables), generated from a set of putative causes $X \in \mathbb{R}^{n \times
d_x}$ (the independent variables), via some unknown linear apparatus $F \in \mathbb{R}^{d_x \times d_y}$.
%
Not all the variables in $X$ exert a causal influence on $Y$.
%
By considering a square binary diagonal matrix of \emph{causal influences} $E
\in \mathbb{D}^{d_x \times d_x}$, we denote by $XE$ the causal factors of $Y$.
%
In summary, the problem can be formalized as:
%
\begin{equation}
    y_i = (x_i E + n_i) F
    \label{eq:model}
\end{equation}
%
where $i$ is a given sample, and $n_i$ is a sample-specific noise drawn from a
centered distribution.
%
While the triplet of variables $X$ and $N$ are independent, we allow each of
them to have any form of covariance.
% what is a general covariance matrix?
%
In practice, we observe $n$ samples $(X, Y)$ from the model.
%
This problem space, along with the sizes of all variables involved, is
illustrated in Figure~\ref{fig:b2b}.
%
Given the model in Equation~\cref{eq:model}, the goal of Back-to-Back
Regression (B2B) is to estimate the matrix of $E$, i.e. to identify the factors
that most reliably account for the multivariate observations.

\subsection{Algorithm}


Back-to-Back Regression (B2B) consists of two steps.
%
First, we estimate the linear regression coefficients $\hat G$ from $Y$ to $X$,
and construct the predictions $\hat X = Y \hat G$.
%
This backward regression recovers the correlations between $Y$ and each factor
of $X$.
%
Second, we estimate the linear regression coefficients $\hat H$ from $X$ to
$\hat X$.
%
The diagonal of the regression coefficients $\hat H$, denoted by $\hat{E} =
\text{diag}(\hat{H})$, is the desired estimate of the causal influence matrix
$E$, as detailed in the \ref{appendix:theorem_proof}.

If using l2-regularized least-squares \citep{hoerl1959optimum, rifkin2007notes},
B2B has a closed form solution:
\begin{align}
    \hat G &= (Y^\top Y + \Lambda_Y)^{-1} Y^\top X,\label{eq:solG}\\
    \hat H &=(X^\top X + \Lambda_X)^{-1} X^\top Y \hat G,\label{eq:solH}
\end{align}
%
where $\Lambda_X$ and $\Lambda_Y$ are two diagonal matrices of regularization
parameters, useful to invert the covariance matrices of $X$ and $Y$ if these are
ill-conditioned.

Performing two regressions over the same data sample can result in overfitting,
as spurious correlations in the data absorbed by the first regression will be
leveraged by the second one.
%
To avoid this issue, we split our sample $(X, Y)$ into two splits $(X_1, Y_1)$
and $(X_2, Y_2)$.
%
Then, the first regression is performed using $(X_1, Y_1)$, and the second
regression is performed using $(X_2, Y_2)$.
%
To compensate for the reduction in sample size caused by the split, B2B is
repeated over many random splits, and the final estimate $\hat E$ of the causal
influence matrix is the average over the estimates associated to each split
\citep{breiman1996bagging}.
%
To accelerate this ensembling procedure, we implemented an efficient
leave-one-out cross-validation scheme as detailed in \citep{rifkin2007notes}
% http://cbcl.mit.edu/publications/ps/MIT-CSAIL-TR-2007-025.pdf,
as follows:
%

\begin{equation}
\hat{Y} = (\Sigma_X G Y - \text{diag}(\Sigma_X G) Y) \;/\; \text{diag}(I - \Sigma_X G) \qquad \text{(element-wise division)}
\end{equation}

where $\Sigma_X$ is the $X$ kernel matrix and where $G$ is computed with an
eigen decomposition of $X$:
\begin{equation}
  \begin{aligned}
  \Sigma_X = Q V Q^T \\
  G = Q (V + \lambda I)^{-1} Q^T\\
  \end{aligned}
\end{equation}
where $Q$, $V$ and $\lambda$ are the eigen vectors, eigen values and
regularization, respectively.

%
% After obtaining $\hat{E}$, we can fit a final regression from $X \hat{E}$ to $Y$.

We summarize the B2B procedure in Algorithm~\ref{algorithm:b2br}.
%
The rest of this section provides a theoretical guarantee on the correctness of
B2B.


% JR: shall we update algorithm with LOO trick?
\begin{algorithm}[H]
    %\SetAlgoLined
    \KwIn{input data $X \in \mathbb{R}^{n \times d_x}$, output data $Y \in \mathbb{R}^{n\times d_y}$, number of repetitions $m \in \mathbb{N}$.}
    \KwOut{estimate of causal influences $\hat{E} \in \mathbb{D}^{d_x \times d_x}$.}
    $\hat{E} \leftarrow 0$\;
    \For{$i = 1, \ldots, m$}{
        $(X, Y) \leftarrow \text{ShuffleRows}((X, Y))$\;
        $(X_1, Y_1), (X_2, Y_2) \leftarrow \text{SplitRowsInHalf}((X, Y))$\;
        $\hat{G} = \text{LinearRegression}(Y_1, X_1)$ \Comment*[r]{$\hat G = (Y_1^\top Y_1 + \Lambda_Y)^{-1} Y_1^\top X_1$}
        $\hat{H} = \text{LinearRegression}(X_2, Y_2 \hat{G})$ \Comment*[r]{$\hat H=(X_2^\top X_2 + \Lambda_X)^{-1} X_2^\top Y_2 \hat G$}
        $\hat{E} \leftarrow \hat{E} + \text{diag}(\hat{H})$\;
    }
    $\hat{E} \leftarrow \hat{E} / m$\;
    $\hat{W} \leftarrow \text{LinearRegression}(X \hat{E}, Y)$\;
    \Return{$\hat{E}$, $\hat{W}$}
    \caption{Back-to-back regression.}
    \label{algorithm:b2br}
\end{algorithm}

\subsection{Theoretical guarantees}
\label{sec:theorem}

\begin{theorem}[B2B consistency - general case]

     Consider the B2B model from Equation $Y = (XE + N)F$, $N$ centered and full
     rank noise.
     %
     Let $Img(M)$ refers to the image of the matrix $M$. If $F$ and $X$ are
     full-rank on the $Img(E)$, then, the solution of B2B, $\hat H$, will
     minimize
     %
     $\min_H  \left \| X - XH\right\| ^2  + \left \| NH\right \| ^2$and satisfy $E\hat H = \hat H$
\end{theorem}
%
\begin{proof}
  See Appendix \ref{appendix:theorem_proof}.
\end{proof}

Since  $E\hat H = \hat H$, we have
\begin{equation}
  \hat H = \arg \min_H  \left \| X - XEH\right\| ^2  + \left \| NEH\right \| ^2 = (E X^\top XE +EN^\top NE) ^\dagger EXX^\top.
\end{equation}
% So, $\hat H = \arg \min_H  \left \| X - XEH\right\| ^2  + \left \| NEH\right \| ^2 = (E X^\top XE +EN^\top NE) ^\dagger EXX^\top$.

Assuming, without loss of generality, that the active features in $E$ are the $k
\in \mathbb{Z}: k \in [0, d_x]$ first features, and rewriting $X=(X_1,X_2)$ and
$N=(N_1,N_2)$ ($X_1$ and $N_1$ containing the $k$ first features), we have:

% $$
\begin{equation}
  X^\top X = \left(\begin{array}{lccl}\Sigma_{X_1 X_1} & \Sigma_{X_1 X_2} \\ \Sigma_{X_1 X_2} & \Sigma_{X_2 X_2}\end{array}\right),\qquad\qquad N^\top N = \left(\begin{array}{lccl}\Sigma_{N_1 N_1} & \Sigma_{N_1 N_2} \\ \Sigma_{N_1 N_2} & \Sigma_{N_2 N_2}\end{array}\right),
\end{equation}

where $\Sigma_{A B}$ is the covariance of $A$ and $B$, and:
% and
\begin{equation}
  \hat H = \left(\begin{array}{cc}(\Sigma_{X_1 X_1}+\Sigma_{N_1 N_1})^{-1}\Sigma_{X_1 X_1} & (\Sigma_{X_1 X_1}+\Sigma_{N_1 N_1})^{-1}\Sigma_{X_1 X_2} \\0 & 0\end{array}\right)
\end{equation}
\begin{equation}
  \text{diag}_k (\hat H) = \text{diag}((\Sigma_{X_1 X_1}+\Sigma_{N_1 N_1})^{-1}\Sigma_{X_1 X_1}) = \text{diag}((I+\Sigma_{X_1 X_1}^{-1}\Sigma_{N_1 N_1})^{-1})
  \label{eq:diagk}
\end{equation}
%
% $$
% $$
% $$
% $$
%
In the absence of noise, we have $\Sigma_{N_1 N_1}=0$, and so
$\text{diag}_k(\hat H)=I$, and $$\text{diag}(\hat H) = \text{diag}(E)$$
Therefore, we recover $E$ from $\hat H$.


In the presence of noise, the causal factors of $E$ correspond to the positive
elements of $\text{diag}(\hat H)$. The methods to recover them are presented in
the Appendix \ref{recovering}.

\section{Experiments}

We perform two sets of experiments to evaluate B2B: one on controlled synthetic
data, and a second one on a real, large-scale magneto-encephalography (MEG)
dataset.
%
We use scikit-learn's PLS and RidgeCV \citep{sklearn} as well as Pyrcca's
regularized canonical component analysis (RegCCA, \citep{bilenko2016pyrcca})
objects to compare B2B against the standard baselines.

\subsection{Synthetic data}
\label{sec:experiment_synthetic}

\input{synthetic.tex}

\subsection{Magnetoencephalography data}
\label{sec:experiment_real}

\input{meg_exps.tex}

\section{Related work}

Forward and cross-decomposition models have been used to identify the causal
contribution of collinear features onto multi-dimensional observations (e.g.
\citep{naselaris2011encoding}). These approaches typically lead to multiple
coefficients for each features (i.e. one per dimension of $Y$ or one per
component respectively). Furthermore, these coefficients can be difficult to
summarize into a single causal estimate. By contrast, B2B quickly (Fig.
\ref{fig:duration}) leads to a single unbiased scalar values $\hat E$ tending
towards 1 and 0 for causal and non-causal features respectively.

A variety of other statistical methods applied to neuroimaging data have been
proposed to clarify what is being represented in brain responses - i.e. what
feature causes specific brain activity. One of the popular linear method is
Representational Similarity Analysis (RSA)
\citep{kriegeskorte2008representational}, and consists in analyzing the
similarity of brain responses associated with specific categorical conditions
(e.g. distinct images), by (1) fitting one-against-all classifiers on each
condition and (2) testing whether these classifiers can discriminate all other
conditions. The resulting confusion matrix is then analyzed in an unsupervised
manner to reveal which conditions lead to similar brain activity patterns. B2B
differs from RSA in that (1) it uses regressions instead of classifications, and
can thus generalize to new items and new contexts and (2) it is fully
supervised.

Finally, CCA has been used in neuroimaging for a variety of purposes such as
denoising and subject alignment \citep{cca_hotelling, de2019multiway}. While CCA
relates to B2B, these two methods diverge in several ways. First, CCA and B2B
have different objectives: CCA aims to find the potentially numerous and poorly
interpretable components where X and Y are maximally correlated, whereas B2B
aims to recover the causal factors from X to Y. Second, B2B is not symmetric
between $X$ and $Y$: it aims to identify specific causal features by first
optimizing over the decoders $G$ and then over $H$. By contrast, CCA is
symmetric between $X$ and $Y$, and aims to find $G$ and $H$ such that they
project $X$ and $Y$ on maximally correlated dimensions. Third, CCA is based an
eigen decomposition of $XH$ and $YG$ - the corresponding canonical components
are thus mixing the $X$ features in way that limit interpretability and
potentially dilute the impact of each feature onto multiple components. In
contrast B2B assesses each feature $X^j$ on a single $Y$ component specifically
selected to maximize signal-to-noise ratio of that feature $j$. Fourth, and
unlike B2B, CCA does not separately optimize two distinct regularization
parameters for $G$ and $H$. Finally, CCA does not use different data splits to
estimate $G$ and $H$. Together, these differences may explain why B2B reliably
outperform CCA on estimating causal influences (Figs. \ref{fig:percondition} and
\ref{fig:auc_plots}).

% I find a bit surprising to mention causality literature in ML at the end. I
% would have expected this in the introduction to convince reviewer that know
% your stuff.



\section{Conclusion}
In this work, we proposed Back-to-Back (B2B) regression, a linear method to
disentangle confounded factors from multidimensional observations.
%
B2B repeatedly performs two successive multidimensional regressions on
independent subsets of the data: the first regression is applied on the output
domain (as in backward decoding), whereas the second regression is applied on
the input domain (as in forward encoding).
%
We provided a theoretical guarantee about the consistency of B2B, and compared
it to several baselines in controlled synthetic experiments.
%
We also applied B2B to a recent brain imaging dataset, analyzing the timing of
brain responses and their connection to word features.
%
We obtained results consistent with prior work in neuroscience literature,
confirming the reliability of B2B for real data analysis.
%

\clearpage
\newpage

\bibliography{paper}


\section{Appendices}
\input{appendix}

\end{document}
