\documentclass{article}

\usepackage[final]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}

\title{Learning causal features with double regression}

\author{%
  Jean-Remi King\\
  Facebook AI\\
  \texttt{email} \\
  \And
  Fran\c{c}ois Charton\\
  Facebook AI\\
  \texttt{fcharton@fb.com}\\
  \And
  David Lopez-Paz\\
  Facebook AI\\
  \texttt{dlp@fb.com}
  \And
  Maxime Oquab\\
  Facebook AI\\
  \texttt{qas@fb.com}
}

\begin{document}

\maketitle

\begin{abstract}
    To be written.
This is an example of citation

A survey in deep learning \citep{deep_learning_nature}. 
\end{abstract}

\section{Inverse problems}
Many causal problems are inverse problems: trying to tell causes from their effects. We are given experimental data (effects), possible explanations (causes), and are tasked to find the causes that account for the observations, knowing that other non causal (contingent, random) factors are at work.

This can be modelled in the following way. We measure a phenomenon Y, via an apparatus F. We are given a set X of possible causal features, some of which are active (account for the observed effects). We note EX the subset of active features (E being a binary diagonal matrix, ie all zeroes except a few ones on the diagonal). Besides EX, the measured phenomenon is influenced by other factors, which we represent as noise N. Assuming that the noise is centred (equal to zero on average) and that dependences can be represented as linear transformations, we can summarize our model by the following equation: $Y = F(EX + N)$. Our goal, given observations of X and Y, is to find E.
 
 **********Description of the neuroscience side of the problem***********
 

\section{Double regression}
A natural approach to inverse problems is anticausal regression, where one tries to predict the different features of X from the observations Y. Because of noise, loss of information in the measuring apparatus (F not being full rank, or well conditioned), possible correlations between features of X, and the fact that only a few of these features account for Y, such an approach will not retrieve X, but some approximation of it, $\hat X$. 

On the other hand, since we are given a statistical model for calculating Y from X, we could, by assuming some properties of N, use the opposite approach, and predict the different features of Y from X. Again this discounts the correlations between features of Y, the fact that not all features of X are active and the effect of noise.

Double regression tries to combine both approaches by conducting first an anticausal regression, and then trying to predict its result $\hat X$ from X. In other words, the anticausal step (from Y to X) provides us with intermediary features, which the causak step tries to predict from X. Since matrices X and Y are likely to be multicollinear, stable estimators are needed, and we use ridge regression. Besides, since conducting two regressions from the same data sample would cause some of the shared noise to be learnt, we split our sample, and use half of it for each of the regressions, therefore "decorrelating" the two steps.

Under our hypotheses (centred noise, E a binary diagonal), and even under heavy noise, experiments suggest that H provides a good approximation to E. In particular, H is close to diagonal, and binary clustering of its diagonal elements using a Sonquist and Morgan statistics, recovers the original E. 

In a nutshell, double regression is performed as follows:
	
	1- Randomly split the data sample
	
	2- From the first half sample, find G such that GY is the regularized least square approximation of X
	
	3- From the second half of the sample, calculate $\hat X=GY$, and find H such that HX is the regularized least square approximation of $\hat X$
	
	4- Using the Sonquist and Morgan criterion, select the largest diagonal elements of H, set them to one and the rest of H to zero
	
	5- Use the resulting matrix as a estimator of E, the active features in the causal process

	6- Alternatively, use recovered E to predict Y from X 


\section{Mathematics}

\subsection{Model formulation, average behaviour}

Our problem may be formulated as
\begin{equation}
    Y = F(EX + N)
    \label{eq:model}
\end{equation}
where F is an unknown matrix, presumably full or high rank, corresponding to the measurement apparatus, E is a low rank transformation carried on the features (X), and N a noise term, centred but not necessarily gaussian or diagonal.

The first regression (X from Y), amounts to finding
\begin{equation}
\begin{aligned}
\min_G \left \| X-GY \right \|^2 &= \min_G \left \| X - GF(EX+N)\right\|^2 \\
&{}= \min_G \left \| (I-GFE)X\right\| ^2 + \left \| GFN\right \| ^2\\
&{}\leq \min_G \left \| I-GFE\right\| ^2 \left \| X\right\| ^2 + k\left \| G\right \| ^2\\ 
\end{aligned}
\end{equation}
since N is centred, and with k a positive constant which depends on the variance of FN.

This amounts to finding the minimal norm generalized inverse of FE, or its Moore-Penrose inverse
\begin{equation}
\hat  G= (FE)^{\dagger}
\end{equation}

The second regression is from X to the predicted value of X in the above, or $\hat G  Y$

\begin{equation}
\begin{aligned}
\min_H \left \| (FE)^{\dagger}Y - HX \right \|^2 &= \min_H \left \| (FE)^{\dagger}F(EX+N) - HX \right \|^2 \\
&= \min_H \left \| ((FE)^{\dagger}(FE)-H)X \right \| ^2 + \left \| (FE)^{\dagger}FN \right \| ^2\\
&= \min_H \left \| ((FE)^{\dagger}(FE)-H)X \right \| ^2\\
\end{aligned}
\end{equation}

Therefore, under the hypothesis that N is centred, our regression method estimates 
\begin{equation}
\begin{aligned}
\hat H =(FE)^{\dagger}(FE)
\end{aligned}
\end{equation}

For any matrix A, $A^\dagger A$ is the orthogonal projector over $Img(A')$, and $I-A^\dagger A$ the orthogonal projector over $Ker(A)$. 

If E selects some features of X (ie $Ex=0$ on any unselected feature, and E full ranked over the set of selected features) and if F is full rank over the subspace spanned by the features selected by E, then 
\begin{equation}
\begin{aligned}
Ker(FE) = Ker(E)  
\end{aligned}
\end{equation}
$I - \hat H$ is the projector over $Ker(E)$, the diagonal matrix that spans the unselected features, and $\hat H$ is the diagonal matrix corresponding to the selected features.

Under such hypotheses, $\hat H$ retrieves the features of X that have a causal influence on Y, and should do it in a fairly resilient way, since it is diagonal, and all its eigenvalues (0 or 1) are well separated.

\subsection{The double regression estimator, and canonical component analysis}

For any real matrices of X and Y, the least square regression of Y from X is calculated by the formula (X' denoting the transpose of X)
\begin{equation}
\begin{aligned}
(X'X)^{-1} X'Y
\end{aligned}
\end{equation}
and the prediction of Y thus obtained (from X) can be written
\begin{equation}
\begin{aligned}
X(X'X)^{-1} X'Y
\end{aligned}
\end{equation}
 
In double regression, we first calculate the regression of X from Y, and calculate $\hat X$ the corresponding prediction as $Y(Y'Y)^{-1} Y'X$. The regression of $\hat X$ from X then becomes
\begin{equation}
\begin{aligned}
(X'X)^{-1} X'Y(Y'Y)^{-1} Y'X
\end{aligned}
\end{equation}
Under ideal conditions (good conditioning of $X'X$ and $Y'Y$), this should recover a diagonal binary matrix, the 1 on the diagonal corresponding the the features of X used by Y. In a noisy environment, double regression tries to extract the useful features of X (ie those having an influence on Y) as non zero elements of the diagonal of (9).

Formula (9) shows that double regression can be understood as a special case of canonical component analysis (CCA). In CCA, given two matrixes (n,p and n,q) X and Y describing different features measured on the same data sample, one tries to characterise the closeness between X and Y (ie between the subspaces spanned by their columns) as the correlation between linear combinations of X and Y. This is done by finding vectors $a$ and $b$ such that $Xa$ and $Yb$ display maximal correlation, or by maximising $a'X'Yb$ over unit-normed $a$ and $b$. To find a, one calculates the eigenvectors of (9). The largest one is the direction with maximal correlation, the second one the maximal correlation direction orthogonal to the first, and so on. For each canonical dimension, the corresponding eigenvalue is the square of the cosine between the corresponding direction along X and the subspace spanned by Y.  

If (9) is diagonal, for instance in the model presented above, the features of X are eigenvectors. If all eigenvalues are either 0 or 1, since they correspond to the squared cosine of the angle between the corresponding feature and the subspace spanned by Y, all the features are either part of the Y subspace (eigenvalues of one) or orthogonal to it (for zero eigenvalues). 

Thus, double regression amounts to CCA under the constraint that the subspaces spanned by X and Y are perpendicular, X is spanned by vectors of Y and vectors orthogonal to Y, and that the parallel and orthogonal components of X are spanned by disjoint sets of features of X. The closer we are to this hypothesis, the closer the results of CCA and double regression will be.

\subsection{Double regression as feature extraction}

Testing for non zero elements on the diagonal of (9) can be viewed as a one dimensional binary clustering problem: classify n positive real values as "large" and "small" (under the hypothesis that they are not all small or all large). This can be done on maximising the ratio of inter-group variance over total variance (ie minimising intra-group variance). In a one dimensional two clusters setting, this amounts to sorting the data, and separating the p smallest from the n-p largest values so that inter group inertia is maximal. 

If s and r are the average values of the two clusters, and p and n-p their size, we are maximising 
\begin{equation}
\begin{aligned}
{p (n-p)\over n^2} (s-r)^2 
\end{aligned}
\end{equation}
 and selecting the p features corresponding to the largest values.

\subsection{Stability analysis}

\subsection{Related approaches}
(PLS, 2 way, 3 fold...)

\section{Experiments - simulated data}
Experiments on simulated data serve three goals: compare double regression with other known techniques (CCA, OLS, PLS...) both as regression and feature extraction methods, provide evidence for our claim on stability, invariance to environment, and behavior as a causal detector, and understand the strengths and limitations of double regression, and directions for future improvement.

\subsection{Experimenting with double regression}
In these experiments, we focus on double regression as a feature detector. Given p features X, exhibiting various levels of correlations and observed on a sample of size n, matrix E extracts a (small) set of d features. A noise term N (p, n) of various covariance and signal to noise ratio (compared to EX) is then added, and the result is transformed by a random (q,p) matrix F, to yield an (q,n) Y.

In data generation, we experiment on the size of feature spaces (p,q), the proportion of selected features (d/p), signal to noise ratios, and the conditioning of X (feature multicollinearity), N (noise covariance) and F. In the algorithm, we test various ways of regularizing the two regressions (namely pseudo inversion of X and Y), of splitting data samples between the regressions, of extracting the selected features, and of using the information recovered to improve prediction of Y.

In all experiments, the figure of merit is the quality of the reconstruction of E, measured as the Hamming distance (total number of misclassifications) between the selection performed by E and the features extracted 

\subsection{Comparison with other methods - feature extraction}

\subsection{Comparison with other methods - predicting Y}

\subsection{Comparison with other methods - stable regression}
In this section, we perform the same tests as in the previous one, but the test set is taken from a different distribution of X than the training set. In practice, we test the same trained predictors of Y over test samples generated fro; a distribution of X with a different covariance.

The rationale behind such tests is that whereas double regression, being a feature extraction method, might not be as efficient as dedicated porediction tools, its focus on causal elements (and the removal of non causal features before carrying out regression) might cause it to be more resistant to changes in environments. 

********** here be results and discussion ************

\section{Experiments}

\section{Conclusion}

\clearpage
\newpage

\bibliographystyle{abbrvnat}
\bibliography{paper}

\end{document}
