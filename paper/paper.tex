\documentclass{article}

\usepackage{amsmath, amsfonts, microtype, xcolor, tikz, graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage[]{neurips_2019}
\DeclareMathOperator{\Tr}{Tr}

\usetikzlibrary{calc}

\tikzset{
    ncbar angle/.initial=90,
    ncbar/.style={
        to path=(\tikztostart)
        -- ($(\tikztostart)!#1!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztotarget)$)
        -- ($(\tikztotarget)!($(\tikztostart)!#1!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztotarget)$)!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztostart)$)
        -- (\tikztotarget)
    },
    ncbar/.default=0.5cm,
}

\tikzset{round left paren/.style={ncbar=0.5cm,out=110,in=-110}}
\tikzset{round right paren/.style={ncbar=0.5cm,out=70,in=-70}}

\newcommand{\dlp}[1]{{\color{red} (DLP: #1)}}
\newcommand{\mo}[1]{{\color{green} (MO: #1)}}
\newcommand{\jrk}[1]{{\color{blue} (JRK: #1)}}
\newcommand{\fc}[1]{{\color{orange} (FC: #1)}}

\title{Discovering causal factors with back-to-back regression}

\author{%
  Jean-Remi King\\
  CNRS\\
  \texttt{email} \\
  \And
  Fran\c{c}ois Charton\\
  Facebook AI\\
  \texttt{fcharton@fb.com}\\
  \And
  David Lopez-Paz\\
  Facebook AI\\
  \texttt{dlp@fb.com}
  \And
  Maxime Oquab\\
  Facebook AI\\
  \texttt{qas@fb.com}
}

\begin{document}

\maketitle

\begin{abstract}
Identifying causes from observations is at the core of science. This endeavor
is particularly challenging when i) potential factors are difficult to
manipulate individually and ii) observations are complex and multi-dimensional.
To address this issue, we introduce ``back-to-back regression'' (BFR), a
method designed to identify, from a set of co-varying factors, the factors
that most plausible account for multidimensional observations. After detailing
the proof of convergence, we show that BFR outperforms least-squares and related
regression techniques, as well as cross-decomposition methods (e.g. canonical
correlation analysis, and partial least squares) on two tasks: causal
identification and out-of-distribution prediction. Finally, we apply BFR to
neuroimaging recordings of 102 subjects reading word sequences. The results
show, as expected, that the early brain responses to words appear to be
specifically caused by low-level visual factors whereas late brain responses
appear to be caused by lexical factors, despite the fact that these factors
covary in the study.
% conclude  open
\end{abstract}

\section{Introduction}

\input{introduction.tex}

\section{Back to back regression}

\begin{figure}[t!]
    \centering
    \begin{tikzpicture}
    \newcommand\posY{0}
    \newcommand\posX{3}
    \newcommand\posE{5}
    \newcommand\posN{7}
    \newcommand\posF{9.5}

    \node[thick, draw=black, minimum height=3cm, minimum width=2cm, fill=yellow!10] (Y) at (\posY, 0){};
    \node[] (eq) at (1.5, 0){$=$};
    \node[] (times) at (4, 0){$\times$};
    \node[thick, draw=black, minimum height=3cm, minimum width=1cm, fill=yellow!10] (X) at (\posX, 0){};
    \node[thick, draw=black, minimum height=1cm, minimum width=1cm, fill=red!10] (E) at (\posE, 0){};

    \draw[fill=white] (\posE - 0.5 + 0.0, 0.5 - 0.0) rectangle (\posE - 0.5 + 0.0 + 0.1, 0.5 - 0.0 - 0.1);
    \draw[fill=black] (\posE - 0.5 + 0.1, 0.5 - 0.1) rectangle (\posE - 0.5 + 0.1 + 0.1, 0.5 - 0.1 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.2, 0.5 - 0.2) rectangle (\posE - 0.5 + 0.2 + 0.1, 0.5 - 0.2 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.3, 0.5 - 0.3) rectangle (\posE - 0.5 + 0.3 + 0.1, 0.5 - 0.3 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.4, 0.5 - 0.4) rectangle (\posE - 0.5 + 0.4 + 0.1, 0.5 - 0.4 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.5, 0.5 - 0.5) rectangle (\posE - 0.5 + 0.5 + 0.1, 0.5 - 0.5 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.6, 0.5 - 0.6) rectangle (\posE - 0.5 + 0.6 + 0.1, 0.5 - 0.6 - 0.1);
    \draw[fill=black] (\posE - 0.5 + 0.7, 0.5 - 0.7) rectangle (\posE - 0.5 + 0.7 + 0.1, 0.5 - 0.7 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.8, 0.5 - 0.8) rectangle (\posE - 0.5 + 0.8 + 0.1, 0.5 - 0.8 - 0.1);
    \draw[fill=black] (\posE - 0.5 + 0.9, 0.5 - 0.9) rectangle (\posE - 0.5 + 0.9 + 0.1, 0.5 - 0.9 - 0.1);

    \node[] (plus) at (6, 0){$+$};
    \node[thick, draw=black, minimum height=3cm, minimum width=1cm, fill=blue!10] (N) at (\posN, 0){};
    \node[thick, draw=black, minimum height=2cm, minimum width=1cm, fill=red!10] (F) at (\posF, 0){};
    \draw [thick] (2.25, -1.5) to [round left paren ] (2.25, 1.5);
    \draw [thick] (7.75, -1.5) to [round right paren ] (7.75, 1.5);

    \node[] (times2) at (8.5, 0){$\times$};

    \node[] (annY) at (\posY, -1.8){\scalebox{0.85}{$Y \in \mathbb{R}^{n \times q}$}};
    \node[] (annX) at (\posX, -1.8){\scalebox{0.85}{$X \in \mathbb{R}^{n \times d}$}};
    \node[] (annE) at (\posE, -1.8){\scalebox{0.85}{$E \in \mathbb{D}^{d \times d}$}};
    \node[] (annN) at (\posN, -1.8){\scalebox{0.85}{$N \in \mathbb{R}^{n \times d}$}};
    \node[] (annF) at (\posF, -1.8){\scalebox{0.85}{$F \in \mathbb{R}^{d \times q}$}};

    \node[] (labY) at (\posY, 2){Observations};
    \node[] (labX) at (\posX, 2){Factors};
    \node[] (labE) at (\posE, 2.3){Cause};
    \node[] (labE) at (\posE, 2){selection};
    \node[] (labN) at (\posN, 2){Noise};
    \node[] (labF) at (\posF, 2.3){Cause-effect};
    \node[] (labF) at (\posF, 2){mapping};

    \node[] (sim1) at (0,-3.25) {\scalebox{0.85}{$X \sim P(X)$}};
    \node[] (sim2) at (0,-3.65) {\scalebox{0.85}{$N \sim P(N)$}};
    \node[] (reg1) at (5,-3.25) {$\hat{E} = \underbrace{(X_2^\top X_2 + \lambda_X)^{-1} X_2^\top Y_2\overbrace{(Y_1^\top Y_1 + \lambda_Y)^{-1} Y_1^\top X_1}^{\text{1) } \hat{X} : \text{ regression from } Y \text{ to } X}}_{\text{2) regression from } X \text{ to } \hat{X}} $};
    \end{tikzpicture}
    \caption{In its linear form, back-to-back regression identifies the subset of factors that suffices to account for multidimensional observations by piping two successive regressions from $Y$ to $X$ and from $X$ to $\hat X$. }
    \label{fig:}
\end{figure}


\subsection{Algorithm}

Back-to-back regression consists of two steps.
%
First, we perform a least squares estimation of X from Y, that is we find $\hat G$ such that $\hat X=\hat G Y$ is the least square predictor of X.
%
This recovers an approximation of the causes of Y in X.
%
Then, we perform a least squares estimation of $\hat X$ from X, and determine $\hat H$ such that $\hat {\hat X}=\hat H X$ is the least square predictor of $\hat X$.
%
In matrix form, we have $\hat G=(Y'Y)^{-1} Y'X$, and
\begin{equation} \hat H=(X'X)^{-1} X'Y(Y'Y)^{-1} Y'X\end{equation}
Under good conditions (close to linear dependence between X and Y, and covariance matrices of X and Y well conditioned), this recovers a scaled approximation to E.

In practice, the covariance matrices of Y and X might exhibit multicollinearity and be badly conditioned, which make their inversion unstable. To guard against this, we compute their singular value decompositions, and truncate their lowest eigenvalues so as to reach a specific condition number in the Frobenius norm. We prefer the truncated SVD approach over the more common regularisation (ridge regression) because it introduces less bias in the estimation. A small amount of regularisation is still used, for numerical stability purposes.

If $X'X=U'DU$, U orthogonal and D diagonal, and D* is D with the lowest diagonal elements set to zero, this amounts to replacing $(X'X)^{-1}$ by $U(D*^{-1}+\epsilon I)U'$, with $\epsilon$ a positive number close to machine floating point accuracy. D is truncated so as to have a certain condition number that is determined by leave one out cross validation.
%
Also, to prevent spurious correlations between the two steps (should they be performed on the same sample), our original sample is randomly split into two halves, and each step is performed over one of them, effectively decorrelating the two steps.
%
This is repeated over several (100) splits, and the resulting $\hat H$ are averaged.
%
This bagging compensates for the reduction in sample size caused by splitting.

Under our hypotheses (centered noise, E a binary diagonal) and even under heavy noise, $\hat H$ is close to diagonal.
%
Yet, because of regularisation which adds positive bias to small eigenvalues, and noise which attenuates large eigenvalues, $\hat H$ is in fact a scaled and noisy approximation to E.
%
To recover a binary diagonal, as our estimate of E, we extract the diagonal of $\hat H$ and select its large elements.
%
The diagonal matrix $\hat E$, having large elements of H as its unit diagonal coefficients (the rest being zero) is our estimate of E, the active features of X.

To select large elements of $\hat H$, the traditional approach in regression analysis consists in testing whether its coefficients differ from zero. This is usually done via variants of Student t-test.
%
This will not work here, as back-to-back regression introduces, through two regularisations, a significant amount of positive bias on the diagonal, which depends in a complex manner on noise, dimensionality and conditioning of X and Y.
%
Instead, we treat feature extraction as a one dimensional binary clustering problem: classify n positive real values as "large" and "small" (under the hypothesis that they are not all small or all large).

This can be done by maximizing the ratio of inter-group variance over total variance (ie minimizing intra-group variance).
%
In a one dimensional two clusters setting, this amounts to sorting the data, and separating the p smallest from the n-p largest values so that inter group inertia is maximal.
%
If s and r are the average values of the two clusters, p and n-p their size and V the total variance of the sample, we are maximizing the Sonquist and Morgan criterion $$K = {p (n-p) \over n} {(s-r)^2 \over V}$$ and selecting the p features corresponding to the largest values.
%
If diagonal elements of $\hat H$ are normally distributed, K follows a chi-square distribution with one degree of freedom.
%
At 95\% confidence level, a value of K superior to 3.84 makes our selection significant (for 99\% confidence, the corresponding value of K is 6.63) \citep{Kass_75}

This extract from $\hat H$ a binary diagonal $\hat E$.
%
To use back-to-back regression as a regression technique (to predict Y from X), we multiply X by $\hat E$ and perform a cross validated ridge regression of each feature of Y from $\hat E X$.

Summarizing, back-to-back regression is performed as follows:
\begin{enumerate}
\item Randomly split the data sample.
\item From the first half sample, using crossalidated ridge regression, find $\hat G$, the regularised least square regression of X from Y: $\hat G=(Y'Y)^{-1} Y'X$.
\item From the second half sample, calculate $\hat X = \hat G Y$, and find $\hat H$, the regularized least square regression of $\hat X$ from X: $\hat H=(X'X)^{-1} X'Y(Y'Y)^{-1} Y'X$.
\item Repeat steps 1 to 3 and average the results as $\hat H$.
\item Using the Sonquist and Morgan criterion, select the large diagonal elements of $\hat H$, set them to one and the rest of $\hat  H$ to zero.
\item Use the resulting matrix $\hat E$ as an estimator of E, the active features in the causal process
\item Alternatively, use $\hat E$ to predict Y from X, by performing a regularised least square regression of Y from $\hat E X$.
%

\end{enumerate}

\subsection{Asymptotic behaviour - linear case}
As discussed in the introduction, our problem can be formulated (up to a redefinition of the features of X) as $Y=f(EX+N)$, with f an unknown function.
%
If F is linear we can rewrite it as $Y = F(EX + N)$, with X an (dx, n) matrix of possible causes, E a (dx,dx) binary diagonal matrix selecting active features of X.
%
N a (dx,n) homoscedastic noise matrix, F an unknown (possibly full rank) (dy,dx) transformation corresponding to the measuring apparatus, and Y a (dy, n) matrix of measured effects.

The two steps of back-to-back regression amounts to finding $\hat G=\arg \min_G \left \| X-GY \right \|^2$ and $\hat H =\arg \min_H \left \| \hat GY - HX \right \|^2$ (all matrix norms henceforth are Frobenius norms).

For the first step, replacing Y, and since N is centred, we have
\begin{equation}
\begin{aligned}
\arg \min_G \left \| X-GY \right \|^2 &= \arg \min_G \left \| X - GF(EX+N)\right\|^2 \\
&{}= \arg \min_G \left \| X-GFEX\right\| ^2  + \left \| GFN\right \| ^2
\end{aligned}
\end{equation}

As N tends to zero, we are searching for the minimal norm solution of $\min_G \left \| X-GFEX\right\| ^2$, which is $X(FEX)^\dagger$ ($M^\dagger$ denoting the pseudo inverse of M).

Let k be the number of active features of E, suppose for notational convenience that they are the first k features of X, and let $FEX$ be of rank k (which means F and X have full rank on the space spanned by the active features of E). Let K be the $\mathbb{R}^{nx\times k}$ diagonal matrix obtained by deleting the rightmost nx-k columns of E. Then, $E=KK'$, and $FEX=(FK)(K'X)$, with $FK$  and $K'X$ of full rank k.

The rank factorisation property of the pseudo inverse (ref) says that, if $A=BC$ with $A\in\mathbb{R}^{m\times n}$, $B\in\mathbb{R}^{m\times k}$ and $C\in\mathbb{R}^{k\times n}$ of rank k, then $A^\dagger=C^\dagger B^\dagger$ and since C is full rank, $C^\dagger=C' (CC')^{-1}$.

Replacing in the expression of the minimal norm solution yields
\begin{equation}
\begin{aligned}
X(FEX)^\dagger &=X((FK)(K'X))^\dagger \\
<<<<<<< HEAD
&=X(K'X)'((K'X)(K'X)')^{-1}(FK)^\dagger \\
&=(XX')K(K'XX'K)^{-1}(FK)^\dagger 
=======
&=X(K'X)'((K'X)(K'X)')^{-1}((FK)'(FK))^{-1}(FK)' \\
&=(XX')K(K'XX'K)^{-1}(K'F'FK)^{-1} K'F'
>>>>>>> 7ae2ca9060b027aaf79ee5c3e0c3af709c6bb728
\end{aligned}
\end{equation}

Let $XX' = \left(\begin{array}{c|c}\Sigma_{1} & \Sigma_{2} \\\hline \Sigma_{2} & \Sigma_{3}\end{array}\right)$, the top left block being $k\times k$. Then, $(K'XX'K)=\Sigma_{1}$ and $(XX')K(K'XX'K)^{-1}=\left(\begin{array}{c}I_{k} \\\hline \Sigma_{2} \Sigma_{1}^{-1}\end{array}\right)$, call S the matrix obtained by adding Nx-k empty columns to this one (or, equivalently multiplying it by K').

Let us also note that $FE=FKK'$, and so $(K'F'FK)^{-1} K'F'FKK'=I_{k}K'$. Therefore, we have $(K'F'FK)^{-1} K'F'=(FE)^\dagger$.

Thus, in the absence of noise, and if F and X have full rank on the subspace spanned by E, we have $G\hat=X(FEX)^\dagger =S (FE)^\dagger$. If the covariance of X is block diagonal, that is if inactive features and active features are not correlated (that is, if causal features do not act as confounders for inactive features), then S=E, and we are retrieving $E (FE)^\dagger$.

Under centred noise, and if (FN)'(FN) have low condition number (a weak assumption given that N is noise), we have $\left \| GFN\right \| ^2 \approx Var(FN) \left \| G\right \| ^2$, and minimisation of $\left \| X-GFEX\right\| ^2  + \left \| GFN\right \| ^2$ will recover of the minimum of the left norm, $\lambda S (FE)^\dagger$, $\lambda$ a positive number below 1.

Replacing, we have $\left \| X-GFEX\right\| ^2 = \left \| X-\lambda S (FE)^\dagger FEX\right\| = \left \| X(I-\lambda S)\right\|$. If the covariance of X is block diagonal (no confounding between active and inactive features of X), this is equal to $(1-\lambda)^2 Var(X_{active}) + Var(X_{inactive})$ (denoting the variance of X over active and inactive features).

For the right part of the sum, we have $GFN=\lambda S (FE)^\dagger FN= \lambda S (FE)^\dagger (FE+F(I-E))N= \lambda S (E +(FE)^\dagger F(I-E))N=\lambda E N $ (FC last part of proof to be done, need some lemma on (FE)+). The square norm is therefore $\lambda^2 Var(N) (k/nx)$.

The total quantity to be minimised, under noise, but with non confounding features in X, F full rank over E, and FN decently conditioned, is $(1-\lambda)^2 Var(X_{active}) + Var(X_{inactive})+\lambda^2 Var(N) (k/nx)$, setting the partial derivative in $\lambda$ to zero, we obtain $\lambda = \frac{Var(X_{active})}{Var(X_{active})+Var(N) (k/nx)}$, and if we suppose that X has comparable variance over all features, this simplifies to $\lambda= \frac {Var(X)}{Var(X)+ Var(N)}=\frac{1}{1+nsr}$ nsr the noise to signal ratio, Var(N) divided by Var(X).

The first step of the back to back regression therefore retrieves $\frac{1}{1+nsr} E (FE)^\dagger$ if active and inactive features of X are not correlated

The second step is straightforward, replacing Y and since N is centred, we have
\begin{equation}
\begin{aligned}
\arg \min_H \left \| \hat GY - HX \right \|^2 &=\arg  \min_H \left \| \hat GF(EX+N) - HX \right \|^2 \\
&=\arg \min_H \left \| (\hat G(FE)-H)X \right \| ^2 + \left \| \hat GFN \right \| ^2\\
&= \arg \min_H \left \| (\hat G(FE)-H)X \right \| ^2\\
&=\hat G (FE)
\end{aligned}
\end{equation}

Thus, in the linear case, back-and-forth regression retrieves $\hat H = \frac{1}{1+nsr} E (FE)^{\dagger}(FE) = \frac{1}{1+nsr} E$.

\subsection{Asymptotic behaviour - non linear case}

\section{Related works}
\subsection{back-to-back regression as a special case of canonical component analysis?}
Formula (1) shows that back-to-back regression may be related to canonical component analysis (CCA).
%
In CCA, we are given two matrices X and Y (or size N,p and N,q) that describe different features measured on the same data sample, and characterize their proximity (that is, between the subspaces spanned by their columns) as the correlation between linear combinations of X and Y.

This is done by finding vectors $a$ and $b$ such that $Xa$ and $Yb$ display maximal correlation, or by maximizing $a'X'Yb$ over unit-normed $a$ and $b$.
%
To find a, one calculates the eigenvectors of Eq. (1).
%
The largest one is the direction with maximal correlation, the second one the maximal correlation direction orthogonal to the first, and so on.
%
For each canonical dimension, the corresponding eigenvalue is the square of the cosine between the corresponding direction along X and the subspace spanned by Y.

If the result of Eq. (1) is diagonal, as in the model presented above, the features of X are eigenvectors.
%
If all eigenvalues are either 0 or 1, then all the features are either part of the Y subspace (eigenvalues of one) or orthogonal to it (for zero eigenvalues) because they correspond to the squared cosine of the angle between the corresponding feature and the subspace spanned by Y, .
%
Thus, back-to-back regression amounts to CCA under the constraint that the subspaces spanned by X and Y are perpendicular, X is spanned by vectors of Y and vectors orthogonal to Y, and that the parallel and orthogonal components of X are spanned by disjoint sets of features of X.
%
The closer we are to this hypothesis, the closer the results of CCA and back-to-back regression will be.

There are differences between CCA and back-to-back regression.
%
First, since our objective is not to characterise the proximity between X and Y, but just to extract causal features, we can dispense with the last step of CCA (diagonalisation of (1)).
%
Second, since we operate in a noisy environment, we use regularisation, introducing bias in the correlation calculations.
%
Finally, whereas CCA wants to leverage all correlations between X and Y, we filter some of them, through splitting and bagging.
%
In other words, whereas we are working from the same formulae as CCA, we use them in different ways, for different purposes

\subsection{Related regression methods}
FC: discuss similar methods of regression, that can be used for feature extraction
And maybe have something about other causal discovery methods

\section{Experiments - simulated data}
\input{synthetic.tex}

\section{Experiments - real case}
Jean Remi and Maxime territory

\input{meg_exps.tex}

\section{Conclusion}

\section{Acknowlegements}
Data were provided (in part) by the Donders Institute for Brain, Cognition and Behaviour.

\clearpage
\newpage

\bibliographystyle{abbrvnat}
\bibliography{paper}

\section{Appendices}

This should hold part of the tests, explanations on simulation, detailed results and stuff on meg data

\end{document}
