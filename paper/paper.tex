\documentclass{article}

\usepackage{amsmath, amsfonts, microtype, xcolor, tikz, graphicx, hyperref, amsthm}
\usepackage[ruled, linesnumbered]{algorithm2e}
\usepackage[]{neurips_2019}

\newtheorem{theorem}{Theorem}

\SetKwComment{Comment}{$\triangleright$\ }{}

\usetikzlibrary{calc}

\tikzset{
    ncbar angle/.initial=90,
    ncbar/.style={
        to path=(\tikztostart)
        -- ($(\tikztostart)!#1!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztotarget)$)
        -- ($(\tikztotarget)!($(\tikztostart)!#1!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztotarget)$)!\pgfkeysvalueof{/tikz/ncbar angle}:(\tikztostart)$)
        -- (\tikztotarget)
    },
    ncbar/.default=0.5cm,
}

\tikzset{round left paren/.style={ncbar=0.5cm,out=110,in=-110}}
\tikzset{round right paren/.style={ncbar=0.5cm,out=70,in=-70}}

\title{Measuring causal influence with\\ back-to-back regression: the linear case}

\author{%
  Jean-Remi King\\
  CNRS\\
  \texttt{email} \\
  \And
  Fran\c{c}ois Charton\\
  Facebook AI\\
  \texttt{fcharton@fb.com}\\
  \And
  David Lopez-Paz\\
  Facebook AI\\
  \texttt{dlp@fb.com}
  \And
  Maxime Oquab\\
  Facebook AI\\
  \texttt{qas@fb.com}
}

\begin{document}

\maketitle

\begin{abstract}
Identifying causes from observations is at the core of science. This endeavor
is particularly challenging when i) potential factors are difficult to
manipulate individually and ii) observations are complex and multi-dimensional.
To address this issue, we introduce ``Back-to-Back'' regression (B2B), a
method designed to measure, from a set of co-varying factors, the causal
influences that most plausibly account for multidimensional observations. After
proving the consistency of B2B, we show that our method outperforms
least-squares and related regression and cross-decomposition techniques (e.g.
canonical correlation analysis and partial least squares) on two tasks: causal
identification and out-of-distribution prediction. Finally, we apply B2B to
neuroimaging recordings of 102 subjects reading word sequences. The results
show that the early and late brain responses caused by low- and high-level
word features respectively are more easily detected with B2B than with standard forward regression.
% conclude  open
\end{abstract}

\section{Introduction}

\input{introduction.tex}

\section{Back to back regression}
\label{sec:algorithm}


\begin{figure}[t!]
    \centering
    \begin{tikzpicture}
    \newcommand\posY{0}
    \newcommand\posX{3}
    \newcommand\posE{5}
    \newcommand\posN{7}
    \newcommand\posF{9.5}

    \node[thick, draw=black, minimum height=3cm, minimum width=2cm, fill=yellow!10] (Y) at (\posY, 0){};
    \node[] (eq) at (1.5, 0){$=$};
    \node[] (times) at (4, 0){$\times$};
    \node[thick, draw=black, minimum height=3cm, minimum width=1cm, fill=yellow!10] (X) at (\posX, 0){};
    \node[thick, draw=black, minimum height=1cm, minimum width=1cm, fill=red!10] (E) at (\posE, 0){};

    \draw[fill=white] (\posE - 0.5 + 0.0, 0.5 - 0.0) rectangle (\posE - 0.5 + 0.0 + 0.1, 0.5 - 0.0 - 0.1);
    \draw[fill=black] (\posE - 0.5 + 0.1, 0.5 - 0.1) rectangle (\posE - 0.5 + 0.1 + 0.1, 0.5 - 0.1 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.2, 0.5 - 0.2) rectangle (\posE - 0.5 + 0.2 + 0.1, 0.5 - 0.2 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.3, 0.5 - 0.3) rectangle (\posE - 0.5 + 0.3 + 0.1, 0.5 - 0.3 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.4, 0.5 - 0.4) rectangle (\posE - 0.5 + 0.4 + 0.1, 0.5 - 0.4 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.5, 0.5 - 0.5) rectangle (\posE - 0.5 + 0.5 + 0.1, 0.5 - 0.5 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.6, 0.5 - 0.6) rectangle (\posE - 0.5 + 0.6 + 0.1, 0.5 - 0.6 - 0.1);
    \draw[fill=black] (\posE - 0.5 + 0.7, 0.5 - 0.7) rectangle (\posE - 0.5 + 0.7 + 0.1, 0.5 - 0.7 - 0.1);
    \draw[fill=white] (\posE - 0.5 + 0.8, 0.5 - 0.8) rectangle (\posE - 0.5 + 0.8 + 0.1, 0.5 - 0.8 - 0.1);
    \draw[fill=black] (\posE - 0.5 + 0.9, 0.5 - 0.9) rectangle (\posE - 0.5 + 0.9 + 0.1, 0.5 - 0.9 - 0.1);

    \node[] (plus) at (6, 0){$+$};
    \node[thick, draw=black, minimum height=3cm, minimum width=1cm, fill=blue!10] (N) at (\posN, 0){};
    \node[thick, draw=black, minimum height=2cm, minimum width=1cm, fill=red!10] (F) at (\posF, 0){};
    \draw [thick] (2.25, -1.5) to [round left paren ] (2.25, 1.5);
    \draw [thick] (7.75, -1.5) to [round right paren ] (7.75, 1.5);

    \node[] (times2) at (8.5, 0){$\times$};

    \node[] (annY) at (\posY, -1.8){\scalebox{0.85}{$Y \in \mathbb{R}^{n \times q}$}};
    \node[] (annX) at (\posX, -1.8){\scalebox{0.85}{$X \in \mathbb{R}^{n \times d}$}};
    \node[] (annE) at (\posE, -1.8){\scalebox{0.85}{$E \in \mathbb{D}^{d \times d}$}};
    \node[] (annN) at (\posN, -1.8){\scalebox{0.85}{$N \in \mathbb{R}^{n \times d}$}};
    \node[] (annF) at (\posF, -1.8){\scalebox{0.85}{$F \in \mathbb{R}^{d \times q}$}};

    \node[] (labY) at (\posY, 2){Observations};
    \node[] (labX) at (\posX, 2){Factors};
    \node[] (labE) at (\posE, 2){Cause};
    \node[] (labE) at (\posE, 1.7){selection};
    \node[] (labN) at (\posN, 2){Noise};
    \node[] (labF) at (\posF, 2){Cause-effect};
    \node[] (labF) at (\posF, 1.7){mapping};

    % \node[] (labY) at (\posY, 2)   {\color{gray} MEG recordings};
    % \node[] (labX) at (\posX, 2)   {\color{gray} stimulus};
    % \node[] (labX) at (\posX, 2.3) {\color{gray} features};
    % \node[] (labE) at (\posE, 2.3) {\color{gray} causal};
    % \node[] (labE) at (\posE, 2)   {\color{gray} features};
    % \node[] (labN) at (\posN, 2)   {\color{gray} breathing\ldots};
    % \node[] (labF) at (\posN, 2.3) {\color{gray} bloodflow,};
    % \node[] (labF) at (\posF, 2)   {\color{gray} MEG helmet};

    \node[] (sim1) at (0,-3.25) {\scalebox{0.85}{$X \sim P(X)$}};
    \node[] (sim2) at (0,-3.65) {\scalebox{0.85}{$N \sim P(N)$}};
    \node[] (reg1) at (5.5,-3.25) {$\hat{E} = \text{Diag}(\underbrace{(X_2^\top X_2 + \Lambda_X)^{-1} X_2^\top Y_2\overbrace{(Y_1^\top Y_1 + \Lambda_Y)^{-1} Y_1^\top X_1}^{\text{1) } \hat{X} : \text{ regression from } Y \text{ to } X}}_{\text{2) regression from } X \text{ to } \hat{X}})$};
    \end{tikzpicture}
    \caption{Back-to-back regression identifies the subset of factors $E_{ii} = 1$ in $X$ that influence some observations $Y$ by 1) regressing from $Y$ to $X$ to obtain $\hat{X}$, and ii) returning the diagonal of the regression coefficients from $X$ to $\hat{X}$.}
    \label{fig:b2b}
\end{figure}

We consider the measurement of multivariate signal $Y$, generated from a set of variables $X$, via some unknown linear apparatus $F$.
%
Not all the variables in $X$ exert a causal influence on $Y$.
%
By considering a square binary diagonal matrix of \emph{causal influences} $E$, we denote by $XE$ the causal factors of $Y$.
%
In summary, our model can be formalized as:
%
\begin{equation}
    Y = (XE + N)F,\label{eq:model}
\end{equation}
%
where $N$ is a centered, unobserved noise variable.
%
While the triplet of variables $X$ and $N$ are independent, we allow each of them to have a general covariance matrix.
%
In practice, we observe $n$ samples $(X, Y)$ from the model.
%
This process, along with the sizes of all variables involved, is illustrated in Figure~\ref{fig:b2b}.
%
Given the model in Equation~\eqref{eq:model}, \textbf{the goal} of Back-to-Back Regression (B2B) is to estimate the matrix of causal influences $E$.

%Without loss of generality, we may assume N to be centred(adding, when necessary, an intercept to the causal features), and redefine features and noise so that the model be written $Y=f(EX+N)$ (f a function, E, X and N matrices). In particular, the addition of "external noise" M, as in $Y=F(EX+N)+M$ can be viewed as a small change in F and N.

%We consider both the linear and non linear case. In the first one, our model is $$Y = F(EX+N)$$, with  $Y\in\mathbb{R}^{n\times ny}$, $F\in\mathbb{R}^{ny\times nx}$, $X\in\mathbb{R}^{nx\times n}$, $N\in\mathbb{R}^{nx\times n}$, and E a binary diagonal square matrice of size nx.

% In the non linear model, $F(EX+N)$ is defined as before, but Y is the result of a non linear transformation of it $$Y=\sigma (F(EX+N))$$

%Under this formalism, we are given samples of Y and X, and are tasked to recover E and use it to predict Y from X.

\subsection{Algorithm}


Back-to-Back Regression (B2B) consists in two steps.
%
First, we estimate the linear regression coefficients $\hat G$ from $Y$ to $X$, and construct the predictions $\hat X = \hat Y \hat G$.
%
This backward regression recovers an approximation of the causes of $Y$ in $X$.
%
Second, we estimate the linear regression coefficients $\hat H$ from $X$ to $\hat X$.
%
The diagonal of the regression coefficients $\hat H$, denoted by $\hat{E} = \text{Diag}(E)$, is the desired estimate of the causal influence matrix $E$.

If using regularized least-squares, arguably the most commonly employed linear regression technique \citep{hoerl1959optimum, rifkin2007notes}, the solution of B2B has a closed form solution:
\begin{align}
    \hat G &= (Y^\top Y + \Lambda_Y)^{-1} Y^\top X,\label{eq:solG}\\
    \hat H &=(X^\top X + \Lambda_X)^{-1} X^\top Y \hat G,\label{eq:solH}
\end{align}
%
where $\Lambda_X$ and $\Lambda_Y$ are two diagonal matrices of regularization parameters, useful to invert the covariance matrices of $X$ and $Y$ if these are ill-conditioned.


% If, in the above formalism, F is invertible with respect to the causal features of X (ie the problem has a solution), and the relation is linear or close to linear, the diagonal of $\hat H$ will be a scaled approximation of the diagonal of E. That is to say, back to back regression will recover the causal features of the process considered, even in the presence of heavy noise N, and without needing to estimate the transformation F.

% In practical cases, the covariance matrices of Y and X will often be badly conditioned (because features of X are related, or measurement produces correlated features of Y). This makes the inversion of $X'X$ and $Y'Y$ sensitive to noise (either from N, from sampling or from computation rounding). To guard against high condition numbers, we have used two methods: truncated SVD (ref) and ridge regression (ref).

% In both, we compute the singular value decompositions of the covariance matrix, transform its diagonal part, and invert it. In truncated SVD, the lowest eigenvalues are set to zero, and the corresponding vectors eliminated from future calculations. In ridge regression, a constant factor is added to all eigenvalues. The regularisation factor and truncation threshold are chosen via leave one out cross validation. In practice, ridge regression tends to provide better stability but introduces a bias in the estimates. Truncated SVD is used to provide better results in low noise cases.

Performing two regressions over the same data sample can result in overfitting, as spurious correlations in the data absorbed by the first regression will be leveraged by the second one.
%
To avoid this issue, we split our sample $(X, Y)$ at random into two equally-sized halves $(X_1, Y_1)$ and $(X_2, Y_2)$.
%
Then, the first regression is performed using $(X_1, Y_1)$, and the second regression is performed using $(X_2, Y_2)$.
%
To compensate for the reduction in sample size caused by the split, B2B is repeated over many random splits, and the final estimate $\hat E$ of the causal influence matrix is the average over the estimates associated to each split \citep{efron1992bootstrap}.
%
After obtaining $\hat{E}$, we can fit a final regression from $X \hat{E}$ to $Y$.

We summarize the B2B procedure in Algorithm~\ref{algorithm:b2br}.
%
The rest of this section provides a theoretical guarantee on the correctness of B2B, and an optional post-processing step to binarize the estimated causal influence matrix.



\begin{algorithm}[H]
    %\SetAlgoLined
    \KwIn{input data $X \in \mathbb{R}^{n \times d_x}$, output data $Y \in \mathbb{R}^{n\times d_y}$, number of repetitions $m \in \mathbb{N}$.}
    \KwOut{estimate of causal influences $\hat{E} \in \mathbb{D}^{d_x \times d_x}$.}
    $\hat{E} \leftarrow 0$\;
    \For{$i = 1, \ldots, m$}{
        $(X, Y) \leftarrow \text{ShuffleRows}((X, Y))$\;
        $(X_1, Y_1), (X_2, Y_2) \leftarrow \text{SplitRowsInHalf}((X, Y))$\;
        $\hat{G} = \text{LinearRegression}(Y_1, X_1)$ \Comment*[r]{$\hat G = (Y_1^\top Y_1 + \Lambda_Y)^{-1} Y_1^\top X_1$}
        $\hat{H} = \text{LinearRegression}(X_2, Y_2 \hat{G})$ \Comment*[r]{$\hat H=(X_2^\top X_2 + \Lambda_X)^{-1} X_2^\top Y_2 \hat G$}
        $\hat{E} \leftarrow \hat{E} + \text{Diag}(H)$\;
    }
    $\hat{E} \leftarrow \hat{E} / m$\;
    $\hat{W} \leftarrow \text{LinearRegression}(X \hat{E}, Y)$\;
    \Return{$\hat{E}$, $\hat{W}$}
    \caption{Back-to-back regression.}
    \label{algorithm:b2br}
\end{algorithm}

\subsection{Theoretical guarantees}
\label{sec:theorem}
% As discussed in the introduction, our problem can be formulated (up to a redefinition of the features of X) as $Y=f(EX+N)$, with f an unknown function.
% %
% If F is linear we can rewrite it as $Y = F(EX + N)$, with X an (dx, n) matrix of possible causes, E a (dx,dx) binary diagonal matrix selecting active features of X.
% %
% N a (dx,n) homoscedastic noise matrix, F an unknown (possibly full rank) (dy,dx) transformation corresponding to the measuring apparatus, and Y a (dy, n) matrix of measured effects.
%
% The two steps of back-to-back regression amounts to finding $\hat G=\arg \min_G \left \| X-GY \right \|^2$ and $\hat H =\arg \min_H \left \| \hat GY - HX \right \|^2$ (all matrix norms henceforth are Frobenius norms).

This section provides a consistency proof for B2BR in the case of low noise.

\begin{theorem}[Low-noise consistency]
    Consider the B2BR model from Equation~\ref{eq:model}.
    %
    Assume that $F$ and $X$ are full-rank on the subspace spanned by active entries in $E$. 
    %
    Then, as the variance of $N$ tends to zero and the sample size tends to infinity, B2BR recovers $E$.
\end{theorem}
\begin{proof}
For centered noise $N$, the first regression satisfies:
\begin{align*}
    \arg \min_G \mathbb{E}[\left \| YG - X \right \|^2] &=   \arg \min_G \mathbb{E}[\left \| X - (XE + N)FG \right\|^2]\\
                                                        &{}= \arg \min_G \mathbb{E}[\left \| X - XEFG\right\| ^2]  + \mathbb{E}[\left \| NFG\right \| ^2].
\end{align*}
%
The solution to this first regression is $\hat{G} = [(XEF)^\top (XEF) + (NF)^\top (NF)]^{-1} (XEF)^\top X$.

The second regression is: 
%
\begin{align*}
    \arg \min_H \mathbb{E}[\| XH - Y \hat{G} \|^2] &=\arg  \min_H \mathbb{E}[\| XH - (XE + N)F \hat G \|^2] \\
    &=\arg \min_H \mathbb{E}[\| X(H - EF \hat G) \| ^2] + \mathbb{E}[\| NF\hat G \| ^2]\\
    &= \arg \min_H \mathbb{E}[\| X(H - EF \hat G) \| ^2]
\end{align*}
%
The solution to this second regression is $H = EF\hat{G}$.
%
Therefore, our estimate of $E$ is:
\begin{equation}
    \hat{E} = EF[(XEF)^\top (XEF) + (NF)^\top (NF)]^{-1} (XEF)^\top X.
    \label{eq:estimate}
\end{equation}
%
By the definition of the Moore-Penrose, if $(NF)^\top (NF)$ is full-rank with $\| (NF)^\top (NF) \| \to 0$, it follows that $\hat{E} = EF(XEF)^\dagger X$.
%
Next, assume that the true causal influence matrix contains $k$ ones arranged as $E = \left(\begin{array}{cc} I_k & 0 \\ 0 & 0 \end{array}\right)$, and let $E = K^\top K$, where $K\in \mathbb{R}^{k \times d_x}$.
%
    Also, let the covariance matrix of $X$ be $X^\top X = \left(\begin{array}{cc}\Sigma_{1} & \Sigma_{2} \\ \Sigma_{3} & \Sigma_{4}\end{array}\right)$, where $\Sigma_1 \in \mathbb{R}^{k\times k}$.
%
Then,
\begin{align*}
    \hat{E} &= EF(XEF)^\dagger X = K^\top K F(XK^\top K F)^\dagger X \stackrel{i)}{=} K^\top (K F) (KF)^\dagger (XK^\top)^\dagger X\\
            &\stackrel{ii)}{=} K^\top K(XK^\top)^\dagger X = E (XK^\top)^\dagger X \stackrel{iii)}{=} E (KX^\top X K^\top)^{-1} KX^\top X\\
            &= E (KX^\top X K^\top)^{-1} (\Sigma_1 \,|\, \Sigma_2)  = E \Sigma_1^{-1} (\Sigma_1 \,|\, \Sigma_2)  
            = \left(\begin{array}{cc} I_k & \Sigma_1^{-1} \Sigma_2 \\ 0 & 0 \end{array}\right),
\end{align*}
where the equalities follow due to i) a full-rank factorization of $XEF$, ii) $KF$ being full-rank yields $(KF)(KF)^\dagger = K$, and iii) $X K^\top$ being full-rank yields $(XK^\top)^\dagger = (KX^\top X K^\top)^{-1} KX^\top$.
%
Therefore, $\text{Diag}(\hat{E})$ is the desired influence causal matrix $E$. 
\end{proof}

We observe that, even in the presence of strong noise, the expression of our estimate \eqref{eq:estimate} involves the left-most multiplication with the true causal influence matrix $E$.
%
This means that our estimate will, in expectation, contain zeros for all features that do not causally influence $Y$.
%
When the variance of $N$ increases, we of course pay a price: since the norm $\| X - XEF\hat{G} \| = \| X - X\hat{H} \|$ increases, this means that the values of $\hat{E}$ associated to the causal factors will be dampened.

%%%%%% Next, let $k$ be the number of ones in the binary diagonal causal influence matrix $E$.
%%%%%% %
%%%%%% That is, $\text{tr}(E) = k$.
%%%%%% %
%%%%%% Without loss of generality, assume that these are the first $k$ variables in $X$.
%%%%%% %
%%%%%% Assume that $XEF$ is of rank $k$, which means that $F$ and $X$ have full rank on the space spanned by the active elements in $E$.
%%%%%% %
%%%%%% Let $K \in \mathbb{R}^{k \times d_{x}}$ be the diagonal matrix obtained by deleting the bottom $d_{x} - k$ rows of $E$.
%%%%%% %
%%%%%% %
%%%%%% First, we factorize $(XEF)^\dagger = ((XK^\top)(KF))^\dagger = (KF)^\dagger (XK^\top)^\dagger$.
%%%%%% %
%%%%%% Second, since $XK^\top$ is full rank, we write $(XK^\top)^\dagger = ((XK^\top)^\top (XK^\top))^{-1}(XK^\top)^\top$.
%%%%%% %
%%%%%% Then, $(XEF)^\dagger X = (KF)^\dagger (KX^\top XK^\top)^{-1} KX^\top X$.
%%%%%% 
%%%%%% %Recall the rank factorisation property of the pseudo-inverse \citep{ben2003generalized}: if $A=BC$ with $A\in\mathbb{R}^{m\times n}$ of rank $k$, $B\in\mathbb{R}^{m\times k}$ and $C\in\mathbb{R}^{k\times n}$, then $A^\dagger=C^\dagger B^\dagger$ and, since $B$ is full rank, $B^\dagger=(B^\top B)^{-1} B^\top$.
%%%%%% %
%%%%%% % Using this property on the minimal norm solution $\hat G$ yields
%%%%%% % \begin{equation}
%%%%%% % \begin{aligned}
%%%%%% % (XEF)^\dagger X&=((XK^\top)(KF))^\dagger X\\
%%%%%% % &=(KF)^\dagger ((XK^\top)^\top (XK^\top))^{-1} (XK^\top)^\top X\\
%%%%%% % &=(KF)^\dagger (KX^\top XK^\top)^{-1} KX^\top X
%%%%%% % \end{aligned}
%%%%%% % \end{equation}
%%%%%% 
%%%%%% Write the covariance of $X$ as $X^\top X = \left(\begin{array}{c|c}\Sigma_{1} & \Sigma_{2} \\\hline \Sigma_{2} & \Sigma_{3}\end{array}\right)$, with the block $\Sigma_1$ being $k\times k$. Then, $(KX^\top X K^\top)=\Sigma_{1}$ and $(KX^\top XK^\top)^{-1} KX^\top X=\left(\begin{array}{c|c}I_{k} &  \Sigma_{1}^{-1} \Sigma_{2}\end{array}\right) =: S$.
%%%%%% 
%%%%%% 
%%%%%% Under centred noise, and if the covariance of NF has low condition number (a weak assumption given that N is noise), we have $\left \| NFG\right \| ^2 \approx Var(NF) \left \| G\right \| ^2$, and minimisation of $\left \| X-XEFG\right\| ^2  + \left \| NFG\right \| ^2$ will recover a scaled version of the noiseless solution, $\lambda (KF)^\dagger S$, $\lambda$ a positive number below one.
%%%%%% 
%%%%%% Replacing in the left part of (ref), we have $X-XEFG=X- XK^\top KF \lambda (KF)^\dagger S= X - \lambda XE=X(I-\lambda E)$. Its square norm is $(1-\lambda)^2 Var(X_{active}) + Var(X_{inactive})$ ($Var(X_{.})$ denoting the variance of X over active and inactive features).
%%%%%% 
%%%%%% For the right part, we have $NFG=NF \lambda (KF)^\dagger S = NK^\top (KF+(I-K)F) \lambda  (KF)^\dagger S = \lambda NES=  \lambda NE$. (Since F is full rank over E, we have $(KF)^\dagger (I-K)F= 0$). The square norm is therefore $\lambda^2 Var(N_{active})$.
%%%%%% 
%%%%%% Thus, with centred and random noise, and with F full rank over E, we are minimising $(1-\lambda)^2 Var(X_{active}) + Var(X_{inactive})+\lambda^2 Var(N_{active})$. Its least value is attained for $\lambda = \frac{Var(X_{active})}{Var(X_{active})+Var(N_{active})}$. Assuming X and N have the same variance over all features (this can always be guaranteed), active subscript can be dropped, and this simplifies to $\lambda= \frac {Var(X)}{Var(X)+ Var(N)}=\frac{1}{1+nsr}$ nsr the noise to signal ratio, Var(N) divided by Var(X).
%%%%%% 
%%%%%% The first step of the back to back regression therefore retrieves $\frac{1}{1+nsr} (KF)^\dagger S$.
%%%%%% 
%%%%%% 
%%%%%% Thus, in the linear case, back to back regression retrieves $\hat H = \frac{1}{1+nsr} (K^\top K F)(KF)^{\dagger} S = \frac{1}{1+nsr} E$.
%%%%%% 
%%%%%% ADD AGAIN:
%%%%%% 
%%%%%% Thus, in the absence of noise, and if F and X have full rank on the subspace spanned by E, we have $G\hat=(XEF)^\dagger X=(KF)^\dagger S$. If the covariance of X is block diagonal, that is if inactive features and active features are not correlated (if the causal features do not serve as confounders for non causal ones), then S=K, and we are retrieving $(KF)^\dagger K$.
%%%=======
%%%
%%%Let $X^\top X = \left(\begin{array}{c|c}\Sigma_{1} & \Sigma_{2} \\\hline \Sigma_{2} & \Sigma_{3}\end{array}\right)$, the top left block being $k\times k$. Then, $(KX^\top X K^\top)=\Sigma_{1}$ and $(KX^\top XK^\top)^{-1} KX^\top X=\left(\begin{array}{cc}I_{k} &  \Sigma_{1}^{-1} \Sigma_{2}\end{array}\right)$, call it S.
%%%
%%%Thus, in the absence of noise, and if F and X have full rank on the subspace spanned by E, we have $G\hat=(XEF)^\dagger X=(KF)^\dagger S$. If the covariance of X is block diagonal, that is if inactive features and active features are not correlated (if the causal features do not serve as confounders for non causal ones), then S=K, and we are retrieving $(KF)^\dagger K$.
%%%
%%%Under centred noise, and if the covariance of NF has low condition number (a weak assumption given that N is noise), we have $\left \| NFG\right \| ^2 \approx Var(NF) \left \| G\right \| ^2$, and minimisation of $\left \| X-XEFG\right\| ^2  + \left \| NFG\right \| ^2$ will recover a scaled version of the noiseless solution, $\lambda (KF)^\dagger S$, $\lambda$ a positive number below one.
%%%
%%%Replacing in the left part of (ref), we have $X-XEFG=X- XK^\top KF \lambda (KF)^\dagger S= X - \lambda XE=X(I-\lambda E)$. Its square norm is $(1-\lambda)^2 Var(X_{active}) + Var(X_{inactive})$ ($Var(X_{.})$ denoting the variance of X over active and inactive features).
%%%
%%%For the right part, we have $NFG=NF \lambda (KF)^\dagger S = NK^\top (KF+(I-K)F) \lambda  (KF)^\dagger S = \lambda NES=  \lambda NE$. (Since F is full rank over E, we have $(KF)^\dagger (I-K)F= 0$). The square norm is therefore $\lambda^2 Var(N_{active})$.
%%%
%%%Thus, with centred and random noise, and with F full rank over E, we are minimising $(1-\lambda)^2 Var(X_{active}) + Var(X_{inactive})+\lambda^2 Var(N_{active})$. Its least value is attained for $\lambda = \frac{Var(X_{active})}{Var(X_{active})+Var(N_{active})}$. Assuming X and N have the same variance over all features (this can always be guaranteed), active subscript can be dropped, and this simplifies to $\lambda= \frac {Var(X)}{Var(X)+ Var(N)}=\frac{1}{1+nsr}$ nsr the noise to signal ratio, Var(N) divided by Var(X).
%%%
%%%The first step of the back to back regression therefore retrieves $\frac{1}{1+nsr} S (KF)^\dagger$.
%%%
%%%The second step is straightforward, replacing Y, we have
%%%\begin{equation}
%%%\begin{aligned}
%%%\arg \min_H \left \| \hat YG - XH \right \|^2 &=\arg  \min_H \left \| (XE + N)F \hat G - XH \right \|^2 \\
%%%&=\arg \min_H \left \| X(EF \hat G - H) \right \| ^2 + \left \| NF\hat G \right \| ^2\\
%%%&= \arg \min_H \left \| X(EF \hat G - H) \right \| ^2\\
%%%&= EF \hat G
%%%\end{aligned}
%%%\end{equation}
%%%
%%%Thus, in the linear case, back to back regression retrieves $\hat H = \frac{1}{1+nsr} (K^\top K F)(KF)^{\dagger} S = \frac{1}{1+nsr} E$.
%%%>>>>>>> 4d110522fbb65b4da60327da2627241785a06113


%The diagonal of $\hat H$ recovered by back to back regression is a scaled approximation of the diagonal of E. Noise tends reduce the large diagonal elements, whereas the bias introduced by regularisation tends to increase the lowest ones.
% \subsection{Asymptotic behaviour - non linear case}

\subsection{Binarizing $\hat{E}$}

On the one hand, we have leveraged B2B to obtain the estimate $\hat{E}$, which is a diagonal matrix with real entries.
%
On the other hand, the true causal influence matrix $E$ is a binary matrix, which hard-selects the causal factors of $Y$ from $X$.
%
In this section, we provide a recipe to binarize $\hat{E}$ to estimate the collection of causal factors.

In regression analysis, the traditional approach to this problem employs a $t$-test to check whether the regression coefficients differ from zero \citep{student1908probable}.
%
However, this test will not succeed here, since the estimate $\hat{E}$ is obtained from a double regression, and any employed regularization will add a bias to the diagonal of $\hat{E}$.
%
Instead, we treat the binarization of $\hat{E}$ as a clustering problem: separate the elements in the diagonal into a group of ``small values'', and a group of ``large values''.
%
More specifically, we propose to maximize the ratio of inter-group variance and to minimize the intra-group variance, over all possible splits of the diagonal into $p$ largest values and $d_x-p$ smallest values.
%
Letting $m_0$ and $m_1$ be the average values of the two clusters, $p$ and $d_x-p$ their size, and $v$ the total variance of the sample, we select the split maximizing the Sonquist-Morgan \citep{sonquist_morgan} criterion $\frac{p(d_x-p)}{d_x} \frac{(m_1 - m_0)^2}{v}$.
%
To binarize $\hat{E}$, set to one all the diagonal entries belonging to the ``large values'' group in the decided split, and setting to zero the rest of the diagonal entries.

\section{Experiments}

We perform two experiments to evaluate the performance of BB2R: one on controlled synthetic data, and a second one on a real, large-scale magneto-encephalography dataset.
%
We use scikit-learn to implement all of our simulations \citep{sklearn}.

\subsection{Synthetic data}
\label{sec:experiment_synthetic}

\input{synthetic.tex}

\subsection{Magnetoencephalography data}
\label{sec:experiment_real}

\input{meg_exps.tex}

\section{Related work}
% scholkopf2016modeling

Back-to-back regression is closely related to Canonical Correlation Analysis \citep{cca_hotelling}.
%
While CCA computes the eigenvector decomposition of \eqref{eq:solH}, while we compute its diagonal.
%
If \eqref{eq:solH} is diagonal, as in our model \eqref{eq:model}, the variables in $X$ are the eigenvectors.
%
If all the associated eigenvalues are either $0$ or $1$, then all the variables are either part of the $Y$ subspace, or orthogonal to it, respectively.
%
This is because these eigenvalues correspond to the squared cosine of the angle between the corresponding variable in $X$ and the $Y$ subspace.
%
Thus, back-to-back regression is equivalent to CCA under the constraint that the subspaces spanned by $X$ and $Y$ are perpendicular, $X$ is spanned by vectors of $Y$ and vectors orthogonal to $Y$, and that the parallel and orthogonal components of $X$ are spanned by disjoint sets of features of $X$.

There are, however, differences between CCA and back-to-back regression.
%
First, since our objective is not to characterise the proximity between $X$ and $Y$, but to measure causal influences, we can disregard the diagonalization of \eqref{eq:solH} performed by CCA.
%
Second, whereas CCA leverages all correlations between $X$ and $Y$, we filter some of them, through splitting and bagging.

B2BR is also related, but different, from usual causal discovery algorithms \citep{peters2017elements} that hypothesize on the direction of causality, or from measures of causal influence requiring time information \citep{granger1969investigating, janzing2013quantifying}.

\section{Conclusion}

\input{conclusion}

% \section{Acknowlegements}
% Data were provided (in part) by the Donders Institute for Brain, Cognition and Behaviour.

\clearpage
\newpage

\bibliographystyle{plain}
\bibliography{paper}

%\section{Appendices}
%
%This should hold part of the tests, explanations on simulation, detailed results and stuff on meg data

\end{document}
