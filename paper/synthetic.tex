Experiments on simulated data serve three purposes: compare back-to-back regression with other known techniques (CCA, OLS, PLS...) both as a regression and a feature extraction method, measure its performance for various values of its parameters and provide evidence for our claims about stability, invariance to environment, and behavior as a causal detector.

The model used for simulation is $Y=f(snr MEX+N)$, with X a random (dx,N) matrix, E a square binary diagonal of dimension dx with the nc last elements equal to 1 (this means that of the dx features of X, only the nc last have some influence on Y), M and N (dz,dx) and (dz,N) random matrices, and f a function from $\mathbb{R}^{dz}$ to $\mathbb{R}^{dy}$, that can be linear or non linear.
%
In the linear case, the model becomes $Y=F(MEX+N)$, with F a (dy,dz) matrix.
%
In the non linear case, F becomes $FSH$ where F and H are matrices of dimensions (dz,dz) and (dy,dz), and S a vector of dz sigmoid transformations.
%
Finally, $snr$ is a real number measuring the amount of noise.

Our simulation therefore depends upon 6 free parameters :
\begin{itemize}
\item dx : number of features of X
\item nc: number of active features (in EX)
\item dz: number of noise features
\item snr: signal to noise ratio
\item dy: number of features in Y
\item N: size of sample
\end{itemize}

Random matrices X, M, N, H and F are populated with centred gaussian random coefficients with unit variance, divided by $ \surd n$ where n is the smallest dimension of the matrix.
%
Since a random square matrix with unit gaussian coefficients of dimension n has eigenvalues between $ \surd n $ and $\surd n $, such  scaling allows for meaningful comparisons when dimensions dx, dy and dz vary.

In the non linear case, sigmoid functions are...
FC: insert comment about scaling sigmoids, and maybe something about uniform coeffs

We evaluate the performance of back-to-back regression in two different roles : as a feature extractor recovering E, and as a regression method, predicting Y from X over a test sample.
%
For feature extraction, the quality of fit is the number of disagreements between our estimator and the diagonal of E (ie, their Hamming distance, which should be minimised).
%
For regression, we use a test set of N additional measurements, and calculate the R-squared ratio of variances (which should be maximised).
%
Performance indicators are calculated by averaging the results of several experiments.

\subsection{Feature extraction - linear case}
All other parameters being equal, the quality of feature extraction, measured as the discrepancy between features extracted by back-to-back regression ($\hat E$) and the diagonal of E, improves with signal to noise ratio.
%
The decrease is usually very steep until a Hamming distance under 2 is achieved.
%
Figure 1 presents a number of such curves for varying values of dx=dy=dz, nc=30 and N=1000.

Figure 1

The distance between $\hat E$ and E incorporates two different errors: false positives, where features not in E are extracted, and false negatives where features in E are not extracted.
%
For low values of the snr, and typical values of nc (much smaller than dx), false positives are high, but they fall faster as snr increases.
%
False negatives begin lower, but decrease more slowly (figure 2).
%
In other words, for each values of the parameters, one may determine a value of the signal to noise ratio, such that snr higher than this threshold allow for good feature extraction.
%
At this level, all the errors are false negatives.

Figure 2

Working from a fixed sample of 1000, and 15 active features, and varying values of dx, dy, dz (from 50 to 200) and snr (from 0.2 to 1), we observe that, all other factors being equal, the quality of feature extraction and regression decrease when dx increases (X has more inactive features), dz increase (more noise dimensions) and snr decreases (less signal, more noise).
%
The dimensionality of Y plays a much smaller role, except for very small values of dy (less than 10).

For feature extraction, back-to-back regression either works very well (Hamming distance less than 2) or very badly, with a sharp change over small variations of the parameters.
%
The following graphs show the boundary between values of dx and dz that allow for features extractions (dx lower than boundary, or dz larger), for different values of dy (level curves on the graph), and different values of signal to noise ratio and number of active variables.

FC: add graphs here, see whether it is better to have nc fixed, or a proportion of dx, note : part of this can be moved to appendix if lack of space

For values of ...
%
in ..., an empirical formula can be provided.
%
back-to-back regression allows for good feature extraction if :

\subsection{Feature extraction - nonlinear case}


\subsection{Regression }
For regression, we want to understand how feature extraction helps predict Y from X.
%
To this effect, we compare four approaches to regression:
\begin{itemize}
\item standard ridge regression of Y from X, where no features are extracted this is our baseline
\item weighted ridge regression, of Y from $\hat H X$, where features of X are weighted according to the diagonal of $\hat H$
\item filtered ridge regression, of Y from $\hat E X$, where active features are first extracted (as above), and Y is predicted from the extracted features only
\item perfect filtered (oracle) ridge regression of Y from EX, where E is supposed known
\end{itemize}
Over a large set of parameters, we observe that filtered regression is on average better than weighted, which is on average better than standard regression.
%
This demonstrates that back-to-back regression can be used as a regression method, over the areas in parameter space where feature extraction performs well.

FC graphs go here

The quality of regression is independent from dy (except for very small values), increase with snr, decreases with dx, and increases with dz if it is larger than dx
FC graphs go here

\subsection{Comparison with other methods}
Maxime territory

\subsection{Multiple environments}
In this section, we perform the same tests as in the previous one, but the test set is taken from a different distribution of X than the training set.
%
In practice, we test the same trained predictors of Y over test samples generated from a distribution of X with a different covariance.

The rationale behind such tests is that whereas back-to-back regression, being a feature extraction method, might not be as efficient as dedicated prediction tools, its focus on causal elements (and the removal of non causal features before carrying out regression) might cause it to be more resistant to changes in environments.

FC: ideally, we'd like to assess the multiple env performance of jrr, and compare it with others.
%
Are multiple environments limited to regression analyses? How should environments be generated (not just variance of X, I expect)?


