Natural sciences are tasked to find, from a set of hypothetical factors, the
minimal subset that suffices to reliably predict novel observations. This
endeavor is impeded by two major challenges.

First, causal and non-causal factors may be numerous and partially correlated.
% In physics,
% for example, one may be challenged to identify whether fusion is caused by a
% change in temperature or a change in pressure, as these two factors may, at
% first, be difficult to manipulate independently. This issue becomes increasingly
% pronounced as the number of potential factors increases.
In neuroscience, for
example, it can be challenging to identify whether word frequency modulates
brain activity during reading. Indeed, the
frequency of words in natural language covaries with other factors such as their
length (short words are more frequent than long words) and their categories
(determinants are more frequent than adverbs)
\citep{kutas2011thirty,pegado2014timing}. Instead of selecting a set of words
that controls for all of these factors simultaneously, it is thus common to use
a \emph{forward} "encoding model", i.e. to fit a linear regression to predict observations
(e.g. brain activity) from a minimal combination of competing factors (e.g.
word length, word frequency), and analytically investigate
the estimated contribution of each factor from the model's coefficients
\citep{friston1994statistical,naselaris2011encoding,weichwald2015causal,
king2018encoding,huth2016natural}.

The second challenge to measuring causal influence is that observations can be
multidimensional.
For example, brain activity is often
recorded with hundreds or thousands of simultaneous measurements via functional
Magnetic Resonance Imaging, magneto-encephalography (MEG) or multiple electro-
physiological probes \citep{friston1994statistical,steinmetz2018challenges}.
The relationship between putative causes and observations is thus often
done by training models in a \emph{backward}
fashion: i.e. from observations to putative causal factors. For example, it is
common to fit a support vector machine across multiple
brain voxels or multiple electrodes to detect the
category of a stimulus independently of common noise sources (e.g. head
movements, eye blink)
\citep{norman2006beyond,cichy2014resolving,
kriegeskorte2008representational, king2018encoding}.

Both \emph{forward} and \emph{backward} modeling have competing benefits and drawbacks.
Specifically, forward modeling disentangles the independent contribution of
correlated factors, but does not combine multidimensional observations. By
contrast, backward modeling combines multiple observations, but does not
disentangle factors that are linearly correlated \citep{weichwald2015causal,
hebart2018deconstructing, king2018encoding}. To combine some of the benefits of forward
and backward modeling, several authors have proposed to use cross-decomposition
techniques such as Partial Least Squares (PLS) and Canonical Correlation
Analysis (CCA) \citep{de2019multiway, bilenko2016pyrcca}. CCA and PLS aim to find, from two sets of
data $X$ and $Y$, the components $H$ and $G$ such that $XH$ and $YG$ are maximally
correlated or maximally covarying respectively.

While CCA and PLS can make use
of multidimensional features and observations, they are not designed
for feature discovery. First, these methods are not not directional: observations
and factors can be assigned to either $X$ or $Y$. Second, these method project $X$
and $Y$ onto a reduced but nonetheless multidimensional space. Third, because
CCA and PLS are based on a generalized eigen decomposition, their resulting
coefficients mix the features of $X$ and $Y$ in a way that makes them notoriously difficult to
interpret \citep{lebart1995statistique}.

Here, we introduce the `back-to-back regression' (B2B), which not only combines
the benefits of forward and backward modeling (Section~\ref{sec:algorithm}), but
can also provide robust, interpretable, unidimensional and unbiased coefficients for
each factor.

After detailing B2B and proving its convergence
(Section~\ref{sec:theorem}), we show with synthetic data that it outperforms
state-of-the-art forward, backward and cross-decomposition techniques in
disentangling causal factors (Section~\ref{sec:experiment_synthetic}). Finally,
we apply B2B to large neuroimaging datasets and reveal that distinct but
linearly-correlated word features lead to distinguishable brain representations
(Section~\ref{sec:experiment_real}).
