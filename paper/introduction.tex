Natural sciences are tasked to find, from a set of hypothetical factors, the minimal subset that suffices to reliably predict novel observations. This endeavor is impeded by two major challenges.

First, causal and non-causal factors may be numerous and collinear. In physics, for example, one may be challenged to identify whether fusion is caused by a change in temperature or in pressure, as these two factors can be difficult to manipulate independently. This issue becomes increasingly pronounced as the number of potential factor increases. In neuroscience, for example, it can be challenging to identify whether the frequency of a word presented on a subject's retina modulates brain activity as this factor covaries with numerous other factors such as word length (short words are more frequent than long words) word category (determinants are more frequent than adverbs), individual letters etc (e.g. words that contain "z" are rare) \cite{kutas2011thirty,pegado2014timing}. Generating a set of words that control for all of these factors simultaneously can be challenging; it is thus common to use forward modeling, i.e. to train a model to predict observations (e.g. brain activity) from a minimal combination of competing factors (e.g. word length, word frequency, etc, e.g. \cite{huth2016natural}), and investigate the resulting parameters of the best model.

The second challenge to measuring causal influence is that observations can be large and complex. The relationship between causes and effects is thus often considered in a backward manner, by training models to maximally predict causes from large and complex observations. For example, brain activity is generally recorded with hundreds of sensors simultaneously. It is thus common to use 'decoding' techniques, by, for example, fitting a support vector machine across multiple sensors to predict the category of a stimulus \cite{cichy2014resolving, king2016brain, kriegeskorte2008representational}.


Both forward and backward modeling have competing benefits and drawbacks. Specifically, forward modeling disentangles the independent contribution of collinear factors, but does not combine multidimensional observations. By contrast, backward modeling combines multiple observations, but does not disentangle collinear factors \cite{weichwald2015causal, hebart2018deconstructing, king2018encoding}.

We introduce the 'back-to-back regression', which combines the benefits of forward and backward modeling (Section~\ref{sec:algorithm}) After detailing the method and proving its convergence (Section~\ref{sec:theorem}), we show with synthetic data that it outperforms state-of-the-art forward, backward and cross-decomposition techniques, both in terms of estimating causal influence and out-of-environment predictions (Section~\ref{sec:experiment_synthetic}). Finally, we apply back-to-back regression to a large neuroimaging dataset and reveal that distinct but collinear word features leads to distinguishable brain responses (Section~\ref{sec:experiment_real}).
