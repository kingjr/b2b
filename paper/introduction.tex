Natural sciences are tasked to find, from a set of hypothetical factors, the minimal subset that suffices to reliably predict novel observations. This endeavor is impeded by two major challenges.

First, causal and non-causal factors may be numerous and collinear. In physics, for example, one may be challenged to identify whether fusion is caused by a change in temperature or a change in pressure, as these two factors may, at first, be difficult to manipulate independently. This issue becomes increasingly pronounced as the number of potential factors increases. In neuroscience, for example, it can be challenging to identify whether the frequency of a word presented on a subject's retina modulates brain activity because word frequency covaries with other factors such as their length (short words are more frequent than long words), their categories (determinants are more frequent than adverbs), their individual letters (e.g. words that contain "z" are rare) and so forth \citep{kutas2011thirty,pegado2014timing}. Generating a set of words that control for all of these factors simultaneously can be challenging; it is thus common to use forward modeling, i.e. to train a model to predict observations (e.g. brain activity) from a minimal combination of competing factors (e.g. word length, word frequency, e.g. \citep{huth2016natural}), and investigate, in the model, the estimated contribution of each factor \citep{friston1994statistical}.

The second challenge to measuring causal influence is that observations can be large and complex. The relationship between causes and effects is thus often considered in a backward manner, by training models to maximally predict causes from large and complex observations. For example, brain activity is generally recorded with hundreds of sensors simultaneously. It is thus common to use 'decoding' techniques, by, for example, fitting a support vector machine across multiple sensors to predict the category of a stimulus \citep{cichy2014resolving, king2016brain, kriegeskorte2008representational, norman2006beyond}.

Both forward and backward modeling have competing benefits and drawbacks. Specifically, forward modeling disentangles the independent contribution of collinear factors, but does not combine multidimensional observations. By contrast, backward modeling combines multiple observations, but does not disentangle collinear factors \cite{weichwald2015causal, hebart2018deconstructing, king2018encoding}. There also exist cross-decomposition techniques such as Partial Least Squares (PLS) and Canonical Correlation Analysis (CCA), but they are notoriously difficult to interpret \citep{lebart1995statistique}.

We introduce the 'back-to-back regression', which combines the benefits of forward and backward modeling (Section~\ref{sec:algorithm}). After detailing the method and proving its convergence (Section~\ref{sec:theorem}), we show with synthetic data that it outperforms state-of-the-art forward, backward and cross-decomposition techniques, both in terms of estimating i) causal influence and ii) out-of-environment predictions (Section~\ref{sec:experiment_synthetic}). Finally, we apply back-to-back regression to a large neuroimaging dataset and reveal that distinct but collinear word features lead to distinguishable brain responses (Section~\ref{sec:experiment_real}).
